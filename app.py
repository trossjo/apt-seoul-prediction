
import streamlit as st
import pandas as pd
import numpy as np
import pydeck as pdk
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import seaborn as sns
import os
import json

# Basic Configuration
st.set_page_config(
    page_title="ì•„íŒŒíŠ¸ ì‹¤ê±°ë˜ê°€ ì˜ˆì¸¡ ëª¨ë¸ ë°œí‘œ",
    page_icon="ğŸ¢",
    layout="wide"
)

# Korean Font Support for Matplotlib (Windows)
if os.name == 'nt':
    plt.rc('font', family='Malgun Gothic')
plt.rc('axes', unicode_minus=False)

# Custom Style
st.markdown("""
    <style>
    .main {
        background-color: #f0f2f6;
    }
    .stMetric {
        background-color: #ffffff;
        padding: 15px;
        border-radius: 10px;
        box-shadow: 0 4px 6px rgba(0,0,0,0.1);
    }
    h1, h2, h3 {
        color: #1f77b4;
    }
    </style>
    """, unsafe_allow_html=True)

# Title
st.title("ğŸ™ï¸ ì„œìš¸ì‹œ ì•„íŒŒíŠ¸ ì‹¤ê±°ë˜ê°€ ì˜ˆì¸¡ ëª¨ë¸ Optimization")
st.markdown("**ëª©í‘œ**: RMSE(í‰ê·  ì œê³±ê·¼ ì˜¤ì°¨) ìµœì†Œí™” (Target: < 14,000)")

# -------------------------------------------------------------------------------------------
# [Load Data Function]
# -------------------------------------------------------------------------------------------
@st.cache_data
def load_data():
    import gdown
    import tempfile
    import os as _os
    
    # Google Driveì—ì„œ ë°ì´í„° ë¡œë“œ (Streamlit Cloud ë°°í¬ìš©)
    # train.csv íŒŒì¼ ID: 1yYgA8I-0VuQhdTAi1hQZVJ_zK9dy4Har
    train_file_id = "1yYgA8I-0VuQhdTAi1hQZVJ_zK9dy4Har"
    train_url = f"https://drive.google.com/uc?id={train_file_id}"
    
    # ì„ì‹œ íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œ í›„ ì½ê¸° (ëŒ€ìš©ëŸ‰ íŒŒì¼ ë°”ì´ëŸ¬ìŠ¤ ìŠ¤ìº” ìš°íšŒ)
    temp_dir = tempfile.gettempdir()
    train_path = _os.path.join(temp_dir, "train.csv")
    
    if not _os.path.exists(train_path):
        gdown.download(train_url, train_path, quiet=False)
    
    train = pd.read_csv(train_path, low_memory=False)
    
    # Simple rename for display (Korean for Presentation)
    cols_mapping = {
        'ì‹œêµ°êµ¬': 'ì‹œêµ°êµ¬',
        'ì•„íŒŒíŠ¸ëª…': 'ì•„íŒŒíŠ¸ëª…',
        'ì „ìš©ë©´ì (ã¡)': 'ì „ìš©ë©´ì ',
        'ê³„ì•½ë…„ì›”': 'ê³„ì•½ë…„ì›”',
        'ê±´ì¶•ë…„ë„': 'ê±´ì¶•ë…„ë„',
        'ì¢Œí‘œX': 'longitude', # Keep for PyDeck
        'ì¢Œí‘œY': 'latitude',  # Keep for PyDeck
        'target': 'ê±°ë˜ê¸ˆì•¡'
    }
    train = train.rename(columns=cols_mapping)
    
    # Derived
    train['í‰ë‹¹ê°€'] = train['ê±°ë˜ê¸ˆì•¡'] / train['ì „ìš©ë©´ì ']

    # Date Derivation (For Plots)
    train['ê³„ì•½ë…„ì›”'] = train['ê³„ì•½ë…„ì›”'].astype(str)
    train['ê±°ë˜ë…„ë„'] = train['ê³„ì•½ë…„ì›”'].str[:4].astype(int)
    train['ê±°ë˜ì›”'] = train['ê³„ì•½ë…„ì›”'].str[4:].astype(int)
    
    # Coordinates cleaning
    train['latitude'] = pd.to_numeric(train['latitude'], errors='coerce')
    train['longitude'] = pd.to_numeric(train['longitude'], errors='coerce')
    train = train.dropna(subset=['latitude', 'longitude'])
    
    # split sigungu for dong
    if 'ì‹œêµ°êµ¬' in train.columns:
        sigungu_split = train['ì‹œêµ°êµ¬'].str.split(' ', expand=True)
        if sigungu_split.shape[1] >= 3:
            train['dong'] = sigungu_split[2]
        else:
            train['dong'] = 'Unknown'
            
    # Pre-format price for Tooltip
    train['unit_price_str'] = train['í‰ë‹¹ê°€'].apply(lambda x: f"{x:,.0f}")
    
    return train

with st.spinner('ë°ì´í„° ë¡œë”© ì¤‘... (ì²˜ìŒ ì‹¤í–‰ ì‹œ ì‹œê°„ì´ ì†Œìš”ë©ë‹ˆë‹¤)'):
    df = load_data()

# Tabs
tab1, tab2, tab3, tab4, tab5 = st.tabs(["ğŸ“Š í”„ë¡œì íŠ¸ ê°œìš”", "ğŸ“ˆ íƒìƒ‰ì  ë°ì´í„° ë¶„ì„ (EDA)", "ğŸ› ï¸ íŠ¹ì„± ê³µí•™ (Feature Eng)", "ğŸ¤– ëª¨ë¸ë§ ì „ëµ", "ğŸš€ ìµœì¢… ê²°ê³¼"])

# -------------------------------------------------------------------------------------------
# [1] Project Overview
# -------------------------------------------------------------------------------------------
with tab1:
    st.header("1. í”„ë¡œì íŠ¸ ê°œìš” ë° ì„±ê³¼")
    
    col1, col2 = st.columns(2)
    with col1:
        st.markdown("### ğŸ¯ í•µì‹¬ ëª©í‘œ")
        st.markdown("""
        - **ê³¼ì œ**: ì„œìš¸ì‹œ ì•„íŒŒíŠ¸ ì‹¤ê±°ë˜ê°€ ì˜ˆì¸¡
        - **í‰ê°€ ì§€í‘œ**: RMSE (Root Mean Squared Error)
        - **ë°ì´í„°**: 
            - `train.csv` (1,118,822ê°œ í–‰)
            - `test.csv` (9,272ê°œ í–‰)
        """)
            
    st.divider()
    
    st.subheader("ğŸ” ë°ì´í„° í’ˆì§ˆ ì ê²€ (Missing Value Analysis)")
    data = df # Use global loaded data
    
    col_mq1, col_mq2 = st.columns([1, 2])
    
    with col_mq1:
        st.markdown("**ê²°ì¸¡ì¹˜ í˜„í™© (Missing Values)**")
        st.markdown("""
        - **ì´ˆê¸° ë°ì´í„°**: ì¢Œí‘œ(X, Y) ë° ì¼ë¶€ ì•„íŒŒíŠ¸ ì •ë³´ ê²°ì¸¡ ì¡´ì¬
        - **ì¡°ì¹˜**: 
            - ì¢Œí‘œ ê²°ì¸¡(X, Y): **Kakao API Geocoding**ìœ¼ë¡œ 100% ë³µì›
            - ë²”ì£¼í˜•(ì•„íŒŒíŠ¸ëª… ë“±): **'Unknown'**ìœ¼ë¡œ ëŒ€ì²´ (ê²°ì¸¡ ìì²´ë¥¼ í•˜ë‚˜ì˜ ì •ë³´ë¡œ í™œìš©)
            - ìˆ˜ì¹˜í˜•(ê±´ì¶•ë…„ë„ ë“±): **ì¤‘ì•™ê°’(Median)**ìœ¼ë¡œ ëŒ€ì²´í•˜ì—¬ ë¶„í¬ ì™œê³¡ ë°©ì§€
        """)
        
    with col_mq2:
        # Visualize Missing Values (Original) - Assuming we want to show 'What it was like' or 'Current State'
        # Since we load cleaned data in load_data, let's pretend or create a dummy series for demonstration if needed, 
        # or better, just show the cleanliness of 'load_data' output or raw checks.
        # However, presentation usually shows "Problem -> Solution". 
        # For now, let's show the columns that *had* issues.
        
        # Checking actual nulls in loaded data (which should be clean now)
        nulls = data.isnull().sum()
        nulls = nulls[nulls > 0]
        
        if len(nulls) > 0:
            st.warning("âš ï¸ í˜„ì¬ ë°ì´í„°ì— ì•„ì§ ê²°ì¸¡ì¹˜ê°€ ë‚¨ì•„ìˆìŠµë‹ˆë‹¤.")
            st.bar_chart(nulls)
        else:
            st.success("âœ… ëª¨ë“  ì¤‘ìš” ë°ì´í„°(ì¢Œí‘œ í¬í•¨) ê²°ì¸¡ì¹˜ ì œê±°/ë³´ì™„ ì™„ë£Œ!")
            
            # Show what WAS missing (Simulation for presentation flow)
            example_nulls = pd.Series({
                'ì¢Œí‘œX': 0, # Fixed
                'ì¢Œí‘œY': 0, # Fixed
                'ë„ë¡œëª…': 0, # Fixed
                'ìœ í˜•': 0
            })
            # st.bar_chart(example_nulls)
    
    with col2:
        st.markdown("### ğŸ† Leaderboard Score ì§„í–‰")
        scores = pd.DataFrame({
            'ë‹¨ê³„': [
                '(RF) ê¸°ë³¸ ëª¨ë¸ + ìœ„ì¹˜ êµ°ì§‘í™” (k=1000)', 
                '(RF) íƒ€ê²Ÿ ë³€ê²½(í‰ë‹¹ê°€) + ì§€ë¦¬ ì •ë³´', 
                '(RF) êµí†µ í”¼ì²˜ ì¶”ê°€ (ë²„ìŠ¤/ì§€í•˜ì² )',
                '(XGB) ëª¨ë¸ ë³€ê²½ (1k/0.03)', 
                '(XGB) íŒŒë¼ë¯¸í„° (3k/0.02)',
                '(XGB) íŒŒë¼ë¯¸í„° (5k/0.02)',
                '(XGB) íŒŒë¼ë¯¸í„° (5k/0.01)',
                '(XGB) êµí†µ í”¼ì²˜ ì„¸ë¶„í™”'
            ],
            'Score (RMSE)': [
                "16,627", 
                "16,179", 
                "16,283", 
                "16,013", 
                "15,403",
                "15,469",
                "15,322",
                "ğŸš€ 15,114"
            ],
            'ë³€í™”': [
                '-', 
                'â–¼ 448 (ì„±ëŠ¥ í–¥ìƒ)', 
                'â–² 104 (ì˜¤íˆë ¤ í•˜ë½)', 
                'â–¼ 270 (XGB ì „í™˜ íš¨ê³¼)', 
                'â–¼ 610 (ìµœê³  ì„±ëŠ¥ ë‹¬ì„±)',
                'â–² (í•™ìŠµë¥  0.02 -> 0.01ì´ ë” ìœ ë¦¬)',
                'â–¼ 147 (ìµœê³  ì„±ëŠ¥ ê°±ì‹ )',
                'â–¼ 208 (êµí†µ ë°€ë„, ê±°ë¦¬ Clip íš¨ê³¼)'
            ]
        })
        st.dataframe(scores, use_container_width=True)
        
        st.info("ğŸ’¡ **Insight**: **êµí†µ ë°€ë„ ì„¸ë¶„í™”(300/500/800m)**ì™€ **ê±°ë¦¬ Clipping**ì´ ëª¨ë¸ì˜ ê³¼ì í•©ì„ ë§‰ê³  ì¼ë°˜í™” ì„±ëŠ¥ì„ í¬ê²Œ ë†’ì˜€ìŠµë‹ˆë‹¤.")



# -------------------------------------------------------------------------------------------
# [2] Preprocessing
# -------------------------------------------------------------------------------------------
with tab2:
    st.header("2. íƒìƒ‰ì  ë°ì´í„° ë¶„ì„ (EDA)")
    
    st.markdown("ë°ì´í„°ì˜ ì£¼ìš” íŒ¨í„´ê³¼ ìƒê´€ê´€ê³„ë¥¼ ë¶„ì„í•˜ì—¬ ëª¨ë¸ë§ ì „ëµì„ ìˆ˜ë¦½í–ˆìŠµë‹ˆë‹¤.")
    st.divider()
    
    # -----------------------------------
    # 1. Price Analysis
    # -----------------------------------
    st.markdown("### ğŸ“ˆ ê°€ê²© ë³€ë™ ë° ì¶”ì„¸ ë¶„ì„")
    col_p1, col_p2 = st.columns(2)
    
    # Price vs Age
    with col_p1:
        st.markdown("#### ğŸ—ï¸ ê±´ì¶•ë…„ë„ë³„ í‰ê·  í‰ë‹¹ê°€")
        if 'ê±´ì¶•ë…„ë„' in df.columns:
            age_price = df.groupby('ê±´ì¶•ë…„ë„')['í‰ë‹¹ê°€'].mean().reset_index()
            age_price = age_price[age_price['ê±´ì¶•ë…„ë„'] > 1900]
            
            fig_age, ax_age = plt.subplots(figsize=(6, 4))
            sns.lineplot(data=age_price, x='ê±´ì¶•ë…„ë„', y='í‰ë‹¹ê°€', ax=ax_age, marker='o', color='#2ca02c')
            ax_age.set_title("ê±´ì¶•ë…„ë„ì— ë”°ë¥¸ í‰ë‹¹ê°€ (Uìí˜• íŒ¨í„´)")
            ax_age.set_ylabel("í‰ê·  í‰ë‹¹ê°€ (ë§Œì›)")
            ax_age.grid(True, linestyle='--', alpha=0.6)
            st.pyplot(fig_age)
        else:
            st.warning("'ê±´ì¶•ë…„ë„' ë°ì´í„° ì—†ìŒ")
            
    # Price vs Time (Transaction)
    with col_p2:
        st.markdown("#### ğŸ“… ê±°ë˜ ì‹œì ë³„ ê°€ê²© ì¶”ì´")
        if 'ê±°ë˜ë…„ë„' in df.columns and 'ê±°ë˜ì›”' in df.columns:
            time_df = df.groupby(['ê±°ë˜ë…„ë„', 'ê±°ë˜ì›”'])['í‰ë‹¹ê°€'].mean().reset_index()
            time_df['ê±°ë˜ì¼ì'] = pd.to_datetime(time_df['ê±°ë˜ë…„ë„'].astype(str) + '-' + time_df['ê±°ë˜ì›”'].astype(str) + '-01')
            
            fig_time, ax_time = plt.subplots(figsize=(6, 4))
            sns.lineplot(data=time_df, x='ê±°ë˜ì¼ì', y='í‰ë‹¹ê°€', ax=ax_time, marker='o', color='#d62728')
            ax_time.set_title("ì‹œê¸°ë³„ í‰ë‹¹ê°€ ë³€ë™ (Time Series)")
            ax_time.set_ylabel("í‰ê·  í‰ë‹¹ê°€ (ë§Œì›)")
            ax_time.grid(True, linestyle='--', alpha=0.6)
            plt.xticks(rotation=45)
            st.pyplot(fig_time)
        else:
            st.warning("'ê±°ë˜ì‹œì ' ë°ì´í„° ì—†ìŒ")
            
    st.info("ğŸ’¡ **Insight**: êµ¬ì¶•(ì¬ê±´ì¶•)ê³¼ ì‹ ì¶•ì˜ ê°€ê²©ì´ ë†’ê³ , 2022ë…„ ì´í›„ í•˜ë½ í›„ ë°˜ë“±í•˜ëŠ” ì¶”ì„¸ê°€ ëšœë ·í•©ë‹ˆë‹¤.")

    st.divider()

    # -----------------------------------
    # 2. Correlation
    # -----------------------------------
    st.markdown("### ğŸ”¥ ì£¼ìš” ë³€ìˆ˜ ìƒê´€ê´€ê³„ (Correlation)")
    st.caption("ëª¨ë¸ í•™ìŠµì— ì‹¤ì œ ê¸°ì—¬ë„ê°€ ë†’ì€ í•µì‹¬ ë³€ìˆ˜ì™€ íƒ€ê²Ÿ(í‰ë‹¹ê°€) ê°„ì˜ ê´€ê³„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.")
    
    # 1. Base Dataframe
    df_corr = df.copy()
    if 'latitude' in df_corr.columns:
        df_corr = df_corr.rename(columns={'latitude': 'ìœ„ë„', 'longitude': 'ê²½ë„'})
    
    # 2. Select Columns Strategy
    target_cols = ['í‰ë‹¹ê°€', 'ê±°ë˜ê¸ˆì•¡']
    selected_cols = []
    
    # Strategy A: Use Feature Importance if available
    fi_path = 'codes/feature_importance.csv'
    if os.path.exists(fi_path):
        try:
            fi_df = pd.read_csv(fi_path)
            # Get top 20 numeric features
            top_features = fi_df['feature'].head(20).tolist()
            # Map fi_df names to df_corr names (e.g., latitude -> ìœ„ë„)
            name_map = {'latitude': 'ìœ„ë„', 'longitude': 'ê²½ë„'}
            top_features = [name_map.get(f, f) for f in top_features]
            
            selected_cols = [c for c in top_features if c in df_corr.columns]
            if selected_cols:
                st.info("ğŸ’¡ **Feature Importance** ìƒìœ„ ë³€ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•„í„°ë§í–ˆìŠµë‹ˆë‹¤.")
        except:
            pass
            
    # Strategy B: Fallback (Filter manually if A failed or empty)
    if not selected_cols:
        # Get all numeric
        all_numeric = df_corr.select_dtypes(include=[np.number]).columns.tolist()
        # Filter out noisy 'k-' columns and IDs
        selected_cols = [c for c in all_numeric if not c.startswith('k-') and 'id' not in c.lower() and 'unnamed' not in c.lower()]
        # If still too many, prioritize essentials
        if len(selected_cols) > 15:
             essentials = ['ì „ìš©ë©´ì ', 'ê±´ì¶•ë…„ë„', 'ê±°ë˜ë…„ë„', 'ê±°ë˜ì›”', 'ì¸µ', 'ìœ„ë„', 'ê²½ë„', 'ì „ì²´ë™ìˆ˜', 'ì „ì²´ì„¸ëŒ€ìˆ˜', 'ì£¼ì°¨ëŒ€ìˆ˜']
             # Keep vars that match essentials roughly
             filtered = [c for c in selected_cols if any(x in c for x in essentials)]
             if len(filtered) >= 3:
                 selected_cols = filtered

    # Ensure targets are included
    final_cols = list(set(selected_cols + target_cols))
    
    # [Fix] Filter for numeric columns only (prevent 'ValueError: could not convert string to float')
    # Feature Importance file might imply 'Dong' or 'Apt' are important (Label Encoded in model), 
    # but here they are Strings. We must skip them for Correlation Matrix.
    numeric_cols_in_df = df_corr.select_dtypes(include=[np.number]).columns.tolist()
    final_cols = [c for c in final_cols if c in df_corr.columns and c in numeric_cols_in_df]
    
    # 3. Plot
    if len(final_cols) > 1:
        # [Presentation] Rename columns for cleaner display (Remove 'k-' prefix)
        display_df = df_corr[final_cols].copy()
        display_df.columns = [c.replace('k-', '').replace('K-', '') for c in display_df.columns]
        
        corr_mat = display_df.corr()
        
        # Sort by correlation with 'í‰ë‹¹ê°€' for better readability
        if 'í‰ë‹¹ê°€' in corr_mat.index:
            sorted_idx = corr_mat['í‰ë‹¹ê°€'].abs().sort_values(ascending=False).index
            corr_mat = corr_mat.loc[sorted_idx, sorted_idx]
            
        fig_corr, ax = plt.subplots(figsize=(10, 8))
        # [Fix] k=1 removes diagonal from mask (Visible Diagonal)
        # Prevents "Empty First Row/Last Col" issue where only upper/lower triangle exists
        mask = np.triu(np.ones_like(corr_mat, dtype=bool), k=1)
        sns.heatmap(corr_mat, annot=True, fmt='.2f', cmap='coolwarm', ax=ax, vmin=-1, vmax=1, mask=mask)
        plt.title("í•µì‹¬ ë³€ìˆ˜ ìƒê´€ê´€ê³„ ë¶„ì„")
        plt.xticks(rotation=45, ha='right')
        st.pyplot(fig_corr)
    else:
        st.warning("ë¶„ì„í•  ë³€ìˆ˜ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")


# -------------------------------------------------------------------------------------------
# [3] Geo Analysis (Visualization)
# -------------------------------------------------------------------------------------------
with tab3:
    st.header("3. íŠ¹ì„± ê³µí•™ ë° ì§€ë¦¬ì  ë¶„ì„ (Feature Eng)")
    
    # ---------------------------------------------------------
    # 1. Preprocessing Section (Moved from Tab 2)
    # ---------------------------------------------------------
    st.markdown("### ğŸ› ï¸ ë°ì´í„° ì „ì²˜ë¦¬ (Data Preprocessing)")
    
    col_pre1, col_pre2 = st.columns(2)
    with col_pre1:
        with st.expander("1. ë¶ˆí•„ìš”í•œ ë³€ìˆ˜ ì œê±° & íƒ€ê²Ÿ ë³€í™˜", expanded=True):
            st.markdown("**1) Feature Selection**: ë…¸ì´ì¦ˆê°€ ë˜ëŠ” 27ê°œ ë³€ìˆ˜ ì œê±° (k-ì „í™”ë²ˆí˜¸, ê´€ë¦¬ë¹„ë¶€ê³¼ë©´ì  ë“±)")
            st.markdown("**2) Target Transformation**: `ì´ ê±°ë˜ê¸ˆì•¡` -> `í‰ë‹¹ê°€`ë¡œ ë³€í™˜ (ì™œê³¡ ë°©ì§€)")
            st.code("train['í‰ë‹¹ê°€'] = train['ê±°ë˜ê¸ˆì•¡'] / train['ì „ìš©ë©´ì ']", language='python')
            
    with col_pre2:
        with st.expander("2. íŒŒìƒ ë³€ìˆ˜ ìƒì„± (Derived Features)", expanded=True):
            st.markdown("- **ì—°ì‹(Age)**: `ê±°ë˜ë…„ë„` - `ê±´ì¶•ë…„ë„` (êµ¬ì¶•/ì‹ ì¶• ì—¬ë¶€)")
            st.markdown("- **ì‹œê³„ì—´(Time)**: `ê±°ë˜ë…„ë„`, `ê±°ë˜ì›”` ìˆ˜ì¹˜í˜• ë³€í™˜")
            st.code("""
data['ì—°ì‹'] = data['ê±°ë˜ë…„ë„'] - data['ê±´ì¶•ë…„ë„']
data['ê±°ë˜ì›”'] = data['ê³„ì•½ë…„ì›”'].str[4:].astype(int)
            """, language='python')
            
    st.divider()
    
    # Preprocessing Metadata Header
    st.markdown("### ğŸ“‹ ìƒì„¸ ì „ì²˜ë¦¬ ë‚´ì—­ (Metadata)")
    
    import json
    try:
        with open('codes/preprocessing_metadata.json', 'r', encoding='utf-8') as f:
            meta = json.load(f)
            
        c1, c2 = st.columns(2)
        with c1:
            with st.expander("ğŸ—‘ï¸ ì œê±°ëœ ë³€ìˆ˜ (Dropped Features)", expanded=False):
                st.write(meta.get('dropped_features', []))
            with st.expander("ğŸ”¡ ì¸ì½”ë”© ë°©ì‹ (Encoding)", expanded=False):
                st.json(meta.get('encoding', {}))
                
        with c2:
            with st.expander("ğŸ§© ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (Imputation)", expanded=False):
                st.json(meta.get('imputation', {}))
            with st.expander("ğŸ ìµœì¢… í•™ìŠµ ë³€ìˆ˜ (Final Features)", expanded=False):
                st.write(meta.get('final_features', []))
            
    except Exception as e:
        st.info("âš ï¸ ì „ì²˜ë¦¬ ë©”íƒ€ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. (í•™ìŠµ í›„ ìƒì„±ë¨: preprocessing_metadata.json)")
        
    st.divider()
    
    st.markdown("### ğŸ§ ì§€ì—­ë³„ ì‹œì„¸ ë¶„ì„ (Data Insight)")
    st.markdown("ë°ì´í„°ë¥¼ ë¶„ì„í•´ë³´ë©´ **ì§€ì—­(êµ¬, ë™)ì— ë”°ë¥¸ í‰ë‹¹ ê°€ê²© í¸ì°¨**ê°€ ë§¤ìš° í½ë‹ˆë‹¤.")
    
    col_a, col_b = st.columns(2)
    with col_a:
        st.markdown("#### ğŸ›ï¸ Top 10 ë¹„ì‹¼ 'êµ¬' (Gu)")
        gu_data = pd.DataFrame({
            'êµ¬ (Gu)': ['ê°•ë‚¨êµ¬', 'ì„œì´ˆêµ¬', 'ì†¡íŒŒêµ¬', 'ìš©ì‚°êµ¬', 'ì„±ë™êµ¬', 'ë§ˆí¬êµ¬', 'ê´‘ì§„êµ¬', 'ë™ì‘êµ¬', 'ì¤‘êµ¬', 'ê°•ë™êµ¬'],
            'í‰ë‹¹ê°€ (ë§Œì›/ã¡)': [1363, 1186, 1052, 1049, 869, 837, 808, 768, 763, 752]
        })
        st.dataframe(gu_data, use_container_width=True)
    
    with col_b:
        st.markdown("#### ğŸ˜ï¸ Top 10 ë¹„ì‹¼ 'ë™' (Dong)")
        dong_data = pd.DataFrame({
            'êµ¬ (Gu)': ['ì¢…ë¡œêµ¬', 'ì¢…ë¡œêµ¬', 'ì¢…ë¡œêµ¬', 'ê°•ë‚¨êµ¬', 'ê°•ë‚¨êµ¬', 'ì„œì´ˆêµ¬', 'ì¤‘êµ¬', 'ì¢…ë¡œêµ¬', 'ì¢…ë¡œêµ¬', 'ì†¡íŒŒêµ¬'],
            'ë™ (Dong)': ['ì‹ ë¬¸ë¡œ2ê°€', 'í‰ë™', 'í™íŒŒë™', 'ì••êµ¬ì •ë™', 'ê°œí¬ë™', 'ë°˜í¬ë™', 'ì…ì •ë™', 'êµë¶ë™', 'êµë‚¨ë™', 'ì ì‹¤ë™'],
            'í‰ë‹¹ê°€ (ë§Œì›/ã¡)': [2317, 2149, 2039, 1730, 1719, 1633, 1627, 1536, 1500, 1468]
        })
        st.dataframe(dong_data, use_container_width=True)

    st.markdown("---")
    st.subheader("ğŸ¤” ì™œ ë‹¨ìˆœíˆ 'ë™ë³„ í‰ê·  ê°€ê²©'ì„ í•™ìŠµì‹œí‚¤ì§€ ì•Šì•˜ë‚˜?")
    st.error("""
    **âŒ ì‹¤í—˜ ê²°ê³¼: ë‹¨ìˆœ 'ë™ë³„ í‰ê·  ê°€ê²©' í”¼ì²˜ ì¶”ê°€ ì‹œ ê³¼ì í•© ë°œìƒ**
    - **Validation RMSE**: 25,800 (ë§¤ìš° ë‚®ìŒ, í•™ìŠµ ë°ì´í„° ë„ˆë¬´ ì˜ ë§ì¶¤)
    - **Leaderboard Score**: 17,414 (ì˜¤íˆë ¤ Baselineë³´ë‹¤ ì„±ëŠ¥ í•˜ë½)
    
    **ì›ì¸**:
    1. **Data Leakage**: íƒ€ê²Ÿ ì •ë³´ë¥¼ ê·¸ëŒ€ë¡œ í”¼ì²˜ë¡œ ì“°ë©´ ëª¨ë¸ì´ ê·¸ ê°’ì—ë§Œ ì˜ì¡´í•˜ê²Œ ë¨.
    2. **í–‰ì •êµ¬ì—­ì˜ í•œê³„**: ê¸¸ í•˜ë‚˜ ì‚¬ì´ë¡œ ë™ì´ ë°”ë€Œì§€ë§Œ ìƒí™œê¶Œì€ ê°™ì€ ê²½ìš°ê°€ ë§ìŒ. í–‰ì •êµ¬ì—­ ì´ë¦„ë³´ë‹¤ëŠ” **'ì‹¤ì œ ë¬¼ë¦¬ì  ìœ„ì¹˜'**ê°€ ì¤‘ìš”í•¨.
    """)
    
    st.success("""
    **âœ… í•´ê²°ì±…: K-Means Geo Clustering**
    - í–‰ì •êµ¬ì—­ ì´ë¦„ ëŒ€ì‹  **ìœ„ë„/ê²½ë„ ì¢Œí‘œ** ìì²´ë¥¼ êµ°ì§‘í™”í–ˆìŠµë‹ˆë‹¤.
    - **1000ê°œì˜ ë¯¸ì„¸ ê·¸ë£¹**ìœ¼ë¡œ ë‚˜ëˆ„ì–´, í–‰ì •êµ¬ì—­ ê²½ê³„ë¥¼ ë„˜ì–´ì„  **'ì‹¤ì§ˆì ì¸ ì…ì§€ ê°€ì¹˜'**ë¥¼ ëª¨ë¸ì´ í•™ìŠµí•˜ë„ë¡ ìœ ë„í–ˆìŠµë‹ˆë‹¤.
    """)
    
    st.markdown("#### ğŸ—ºï¸ 1000ê°œ í´ëŸ¬ìŠ¤í„° ì‹œê°í™” (K-Means)")
    st.markdown("ì„œìš¸ì‹œ ì•„íŒŒíŠ¸ë¥¼ 1000ê°œì˜ ë¯¸ì„¸ ìƒí™œê¶Œìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì‹œê°í™”í•œ ê²°ê³¼ì…ë‹ˆë‹¤.")
    
    # Run KMeans for Visualization (Cached)
    @st.cache_data
    def run_kmeans(data_df):
        coords = data_df[['longitude', 'latitude']]
        kmeans = KMeans(n_clusters=1000, random_state=42, n_init=10)
        data_df['cluster'] = kmeans.fit_predict(coords)
        return data_df

    with st.spinner('ì§€ë¦¬ì  í´ëŸ¬ìŠ¤í„°ë§ ê³„ì‚° ì¤‘...'):
        geo_df = run_kmeans(df.sample(50000, random_state=42).copy()) # Sample for map speed

    # Calculate mean price per cluster for color intensity
    cluster_stats = geo_df.groupby('cluster')['í‰ë‹¹ê°€'].mean().reset_index()
    cluster_stats.columns = ['cluster', 'mean_price']
    
    # Normalize price for color mapping (0 to 255)
    cluster_stats['norm_price'] = (cluster_stats['mean_price'] - cluster_stats['mean_price'].min()) / \
                                  (cluster_stats['mean_price'].max() - cluster_stats['mean_price'].min())
    
    geo_df = geo_df.merge(cluster_stats, on='cluster')
    
    # ---------------------------------------------------------------------------------------
    # Map Visualization: Side-by-Side Comparison
    # ---------------------------------------------------------------------------------------
    st.markdown("#### ğŸ—ºï¸ 1000ê°œ í´ëŸ¬ìŠ¤í„° vs í‰ë‹¹ ê°€ê²© ì‹œê°í™”")
    st.markdown("ì¢Œì¸¡ì€ **í´ëŸ¬ìŠ¤í„° êµ¬ë¶„**, ìš°ì¸¡ì€ **í‰ë‹¹ ê°€ê²©**ì…ë‹ˆë‹¤. í´ëŸ¬ìŠ¤í„°ê°€ ê°€ê²© ë¶„í¬ë¥¼ ì–¼ë§ˆë‚˜ ì˜ ë°˜ì˜í•˜ëŠ”ì§€ ë¹„êµí•´ë³´ì„¸ìš”.")

    # 1. Prepare Colors for Cluster Map (Random)
    np.random.seed(42)
    cluster_colors = {c: np.random.randint(0, 255, 3).tolist() for c in geo_df['cluster'].unique()}
    colors = geo_df['cluster'].map(cluster_colors).tolist()
    geo_df['c_r'] = [c[0] for c in colors]
    geo_df['c_g'] = [c[1] for c in colors]
    geo_df['c_b'] = [c[2] for c in colors]

    # 2. Prepare Colors for Price Map (Red-Blue Heatmap)
    geo_df['p_r'] = (geo_df['norm_price'] * 255).astype(int)
    geo_df['p_g'] = 50
    geo_df['p_b'] = ((1 - geo_df['norm_price']) * 255).astype(int)

    # Common View State
    view_state = pdk.ViewState( latitude=37.5665, longitude=126.9780, zoom=10, pitch=45, bearing=0 )

    # Common Tooltip (Apply validation logic inside load_data, assuming passed)
    tooltip = {
        "html": """
        <div style="padding: 10px; color: black; background-color: white; border-radius: 5px; box-shadow: 0 2px 4px rgba(0,0,0,0.3); font-family: sans-serif;">
            <b>ğŸ¢ ì•„íŒŒíŠ¸:</b> {ì•„íŒŒíŠ¸ëª…}<br/>
            <b>ğŸ“ êµ¬ì—­(Dong):</b> {dong}<br/>
            <b>ğŸ’° í‰ë‹¹ê°€:</b> {unit_price_str} ë§Œì›/ã¡<br/>
            <b>ğŸ§© í´ëŸ¬ìŠ¤í„°:</b> Group {cluster}
        </div>
        """,
        "style": {"color": "black"}
    }
    
    # Layout Columns
    map_col1, map_col2 = st.columns(2)

    # --- LEFT MAP: Cluster Groups ---
    with map_col1:
        st.subheader("ğŸ§© í´ëŸ¬ìŠ¤í„° ê·¸ë£¹ (Geo Groups)")
        layer_cluster = pdk.Layer(
            "ScatterplotLayer", geo_df,
            get_position=['longitude', 'latitude'],
            get_color=['c_r', 'c_g', 'c_b'],
            get_radius=80, pickable=True, opacity=0.7, stroked=True, filled=True, line_width_min_pixels=0
        )
        r_cluster = pdk.Deck( layers=[layer_cluster], initial_view_state=view_state, tooltip=tooltip, map_style='mapbox://styles/mapbox/light-v9' )
        st.pydeck_chart(r_cluster)
        st.caption("ğŸ¨ Random Colors: 1000ê°œì˜ êµ¬ì—­ êµ¬ë¶„")

    # --- RIGHT MAP: Price Level ---
    with map_col2:
        st.subheader("ğŸ’¸ í‰ë‹¹ ê°€ê²© (Price Level)")
        layer_price = pdk.Layer(
            "ScatterplotLayer", geo_df,
            get_position=['longitude', 'latitude'],
            get_color=['p_r', 'p_g', 'p_b'],
            get_radius=80, pickable=True, opacity=0.7, stroked=True, filled=True, line_width_min_pixels=0
        )
        r_price = pdk.Deck( layers=[layer_price], initial_view_state=view_state, tooltip=tooltip, map_style='mapbox://styles/mapbox/light-v9' )
        st.pydeck_chart(r_price)
        st.caption("ğŸ”´ Red: High Price, ğŸ”µ Blue: Low Price")
    
    st.markdown("### ğŸ“ íšŒì „ ì¢Œí‘œê³„ (Rotated Coordinates)")
    st.markdown("íŠ¸ë¦¬ ëª¨ë¸ì´ ëŒ€ê°ì„  ê²½ê³„ë¥¼ ë” ì˜ í•™ìŠµí•˜ê¸° ìœ„í•´ ì¢Œí‘œê³„ë¥¼ 45ë„ íšŒì „ì‹œí‚¨ íŠ¹ì„±ì„ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")
    st.latex(r"x_{new} = lat + lon, \quad y_{new} = lat - lon")

    # ---------------------------------------------------------------------------------------
    # ---------------------------------------------------------------------------------------
    # 3. Transportation Strategy (Storytelling)
    # ---------------------------------------------------------------------------------------
    st.markdown("---")
    st.header("3. Feature Engineering: êµí†µ ì ‘ê·¼ì„± ê°•í™” ì „ëµ")
    
    st.markdown("""
    ì´ˆê¸° ëª¨ë¸ ë¶„ì„ ê²°ê³¼, ë‹¨ìˆœí•œ **'ê°€ì¥ ê°€ê¹Œìš´ ì—­ê¹Œì§€ì˜ ê±°ë¦¬'**ë§Œìœ¼ë¡œëŠ” ì„œìš¸ì˜ ë³µì¡í•œ êµí†µ ì…ì§€ë¥¼ ì„¤ëª…í•˜ê¸° ì–´ë ¤ì› ìŠµë‹ˆë‹¤.
    ì´ì— ë”°ë¼ **'ë°˜ê²½ ë‚´ ë°€ë„(Density)'**ì™€ **'ê±°ë¦¬ ì œí•œ(Clipping)'** ê¸°ë²•ì„ ë„ì…í•˜ì—¬ ì„±ëŠ¥ì„ ê·¹ëŒ€í™”í–ˆìŠµë‹ˆë‹¤.
    """)
    
    col_str1, col_str2 = st.columns(2)
    
    with col_str1:
        st.error("âŒ ê¸°ì¡´ ì ‘ê·¼ ë°©ì‹ (Baseline)")
        st.markdown("""
        - **ë‹¨ìˆœ ê±°ë¦¬ (Nearest Distance)**: 
            - ë‹¨ìˆœíˆ ê°€ì¥ ê°€ê¹Œìš´ ì—­ê¹Œì§€ì˜ ê±°ë¦¬ë§Œ ê³„ì‚°
            - **ë¬¸ì œì **: 10km ë–¨ì–´ì§„ ì—­ë„ 'ê°€ì¥ ê°€ê¹Œìš´ ì—­'ìœ¼ë¡œ ì¸ì‹ë˜ì–´ ì§‘ê°’ì— ë¶ˆí•„ìš”í•œ ë…¸ì´ì¦ˆ ë°œìƒ (Outlier)
        - **ë‹¨ìˆœ ê°œìˆ˜**:
            - ë°˜ê²½ ê³ ë ¤ ì—†ì´ ì—­ì„¸ê¶Œ ìœ ë¬´ë§Œ íŒë‹¨
        """)
        
    with col_str2:
        st.success("âœ… ê°œì„ ëœ ì ‘ê·¼ ë°©ì‹ (Advanced)")
        st.markdown("""
        - **ê±°ë¦¬ Clipping (Distance Clipping)**:
            - ë²„ìŠ¤: `2km`, ì§€í•˜ì² : `5km` ì´ìƒì€ **ë™ì¼í•˜ê²Œ ë¨¼ ê²ƒìœ¼ë¡œ ê°„ì£¼** (ì˜í–¥ë ¥ ì°¨ë‹¨)
        - **ë©€í‹° ë°˜ê²½ ë°€ë„ (Multi-Radius Density)**:
            - **300m** (ì´ˆì—­ì„¸ê¶Œ), **500m** (ì—­ì„¸ê¶Œ), **800m** (ë„ë³´ê¶Œ) ë‚´ ê°œìˆ˜ë¥¼ ê°ê° ì‚°ì¶œí•˜ì—¬ ì…ì§€ ê°€ì¹˜ ì„¸ë¶„í™”
        - **ê°€ì¤‘ì¹˜ ì ìˆ˜ (Weighted Score)**:
            - ì£¼ìš” ë…¸ì„ (2í˜¸ì„ , 9í˜¸ì„  ë“±)ì— ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•´ ë‹¨ìˆœ ê°œìˆ˜ë³´ë‹¤ ì§ˆì ì¸ ê°€ì¹˜ë¥¼ ë°˜ì˜
        """)

    # Load Transport Data for Visualization (Google Drive)
    try:
        import gdown
        import tempfile
        import os as _os
        temp_dir = tempfile.gettempdir()
        
        # ë²„ìŠ¤ íŒŒì¼ ID: 1kObluIdbX0MnaWhoWn_i6PWRcEojf5id
        bus_path = _os.path.join(temp_dir, "bus_feature.csv")
        if not _os.path.exists(bus_path):
            gdown.download("https://drive.google.com/uc?id=1kObluIdbX0MnaWhoWn_i6PWRcEojf5id", bus_path, quiet=False)
        bus_df = pd.read_csv(bus_path)
        
        # ì§€í•˜ì²  íŒŒì¼ ID: 15w1lH8jb1xtlT-qmn5CIEc3xIfDwFkmH
        sub_path = _os.path.join(temp_dir, "subway_feature.csv")
        if not _os.path.exists(sub_path):
            gdown.download("https://drive.google.com/uc?id=15w1lH8jb1xtlT-qmn5CIEc3xIfDwFkmH", sub_path, quiet=False)
        sub_df = pd.read_csv(sub_path)
        
        # Clean Coords
        bus_df = bus_df.rename(columns={'Xì¢Œí‘œ': 'lon', 'Yì¢Œí‘œ': 'lat'})
        bus_df = bus_df.dropna(subset=['lat', 'lon'])
        sub_df = sub_df.rename(columns={'ê²½ë„': 'lon', 'ìœ„ë„': 'lat'})
        sub_df = sub_df.dropna(subset=['lat', 'lon'])

        st.markdown("### ğŸ—ºï¸ êµí†µ ì¸í”„ë¼ ì‹œê°í™” (Infrastructure Map)")
        
        col_t1, col_t2 = st.columns([3, 1])
        with col_t1:
            view_state_trans = pdk.ViewState(latitude=37.5665, longitude=126.9780, zoom=10.5, pitch=0)
            
            # Layer: Subway (Blue)
            layer_sub = pdk.Layer(
                "ScatterplotLayer", sub_df,
                get_position=['lon', 'lat'],
                get_color=[0, 0, 255, 180],
                get_radius=150,
                pickable=True
            )
            
            # Layer: Bus (Green, smaller)
            layer_bus = pdk.Layer(
                "ScatterplotLayer", bus_df,
                get_position=['lon', 'lat'],
                get_color=[0, 255, 100, 80],
                get_radius=30,
                pickable=True
            )
            
            r_trans = pdk.Deck(
                layers=[layer_bus, layer_sub],
                initial_view_state=view_state_trans,
                map_style='mapbox://styles/mapbox/light-v9',
                tooltip={"html": "<b>{ì—­ì‚¬ëª…}</b><br/>{í˜¸ì„ }" if 'ì—­ì‚¬ëª…' in sub_df.columns else "Transport"}
            )
            st.pydeck_chart(r_trans)
            
        with col_t2:
            st.info("ë°ì´í„° ë¶„í¬ í™•ì¸")
            st.caption(f"ğŸš‡ ì§€í•˜ì² ì—­: {len(sub_df):,}ê°œ")
            st.caption(f"ğŸšŒ ë²„ìŠ¤ì •ë¥˜ì¥: {len(bus_df):,}ê°œ")
            st.markdown("---")
            st.write("ì„œìš¸ ì „ì—­ì— ì´˜ì´˜íˆ ë¶„í¬ëœ ë²„ìŠ¤ ì •ë¥˜ì¥ê³¼ ì£¼ìš” ê±°ì ì¸ ì§€í•˜ì² ì—­ì˜ ë¶„í¬ë¥¼ í™•ì¸í–ˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        st.warning(f"êµí†µ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")

# ---------------------------------------------------------------------------------------


# -------------------------------------------------------------------------------------------
# [5] Modeling Strategy
# -------------------------------------------------------------------------------------------
with tab4:
    st.header("4. ëª¨ë¸ë§ ë° íƒ€ê²Ÿ ìµœì í™” ì „ëµ")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("ğŸ’¡ Unit Price (í‰ë‹¹ ë‹¨ê°€) ì „ëµ")
        st.markdown("**'ì´ ê±°ë˜ê¸ˆì•¡' ëŒ€ì‹  'ì „ìš©ë©´ì ë‹¹ ë‹¨ê°€'ë¥¼ ì˜ˆì¸¡**í•©ë‹ˆë‹¤.")
        

        st.markdown("""
        - í‰í˜•ë³„ ê°€ê²© ì°¨ì´ê°€ í¬ë¯€ë¡œ, **ë‹¨ìœ„ ë©´ì ë‹¹ ê°€ê²©(Target)ì„ ì˜ˆì¸¡**í•˜ê³ ,
        - ë‚˜ì¤‘ì— `ì „ìš©ë©´ì `ì„ ê³±í•´ `ì´ ê±°ë˜ê¸ˆì•¡`ì„ ë³µì›í•˜ëŠ” ì „ëµì´ ì˜¤ì°¨ë¥¼ ì¤„ì…ë‹ˆë‹¤.
        """)
        
    with col2:
        st.subheader("ğŸ“… ê²€ì¦ ì „ëµ (Time Series Split)")
        st.error("âŒ ê¸°ì¡´: Random Split (20%)")
        st.caption("ê³¼ê±°ì™€ ë¯¸ë˜ ë°ì´í„°ê°€ ì„ì—¬ ì‹œê³„ì—´ íŠ¹ì„± ë°˜ì˜ ë¶ˆê°€ (Leakage ìœ„í—˜)")
        
        st.success("âœ… ë³€ê²½: ìµœê·¼ 3ê°œì›” ë¶„ë¦¬ (Time Cutoff)")
        st.markdown("""
        ```text
        [í•™ìŠµ ë°ì´í„° (Train)] (~ 2023.06)      | [ê²€ì¦ (Val)] (2023.07 ~ 09)
        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | â–‘â–‘â–‘â–‘â–‘â–‘
                                            â–² Cutoff Point
        ```
        """)
        st.caption("ë¯¸ë˜ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê³¼ì œ íŠ¹ì„±ì— ë§ì¶° **'ê³¼ê±° ë°ì´í„°ë¡œ í•™ìŠµ -> ë¯¸ë˜ ë°ì´í„°ë¡œ ê²€ì¦'**í•˜ëŠ” íŒŒì´í”„ë¼ì¸ êµ¬ì¶•")

    st.divider()
    
    st.markdown("### ğŸ¤– ëª¨ë¸ ì„ íƒ (Model Selection)")
    
    st.info("""ì´ˆê¸°ì—ëŠ” Baselineì˜ Random Forestë¥¼ ì‚¬ìš©í–ˆìœ¼ë‚˜, ì•™ìƒë¸” ì‹¤í—˜(RF + XGBoost + LightGBM) ê²°ê³¼ 
    ì˜¤íˆë ¤ ì„±ëŠ¥ì´ ì €í•˜ë˜ì–´ **XGBoost ë‹¨ì¼ ëª¨ë¸**ì— ì§‘ì¤‘í•˜ëŠ” ì „ëµìœ¼ë¡œ ì „í™˜í–ˆìŠµë‹ˆë‹¤.""")
    
    col_m1, col_m2, col_m3 = st.columns(3)
    
    with col_m1:
        st.markdown("#### 1. Random Forest")
        st.markdown("**ì—­í• **: Baseline (ì´ˆê¸° ì ‘ê·¼)")
        st.markdown("""
        **ì¥ì **:
        - ê³¼ì í•©ì— ê°•í•¨ (ë¶„ì‚°â†“)
        - ì•ˆì •ì ì¸ ì„±ëŠ¥ ë³´ì¥
        - ì—¬ëŸ¬ íŠ¸ë¦¬ì˜ í‰ê· ìœ¼ë¡œ ì˜ˆì¸¡
        
        **ë‹¨ì **:
        - ê° íŠ¸ë¦¬ê°€ ë…ë¦½ì ìœ¼ë¡œ í•™ìŠµ
        - **ì”ì—¬ ì˜¤ì°¨(Residual)ë¥¼ ì§‘ìš”í•˜ê²Œ í•™ìŠµí•˜ì§€ ëª»í•¨**
        - ì„±ëŠ¥ í•œê³„ ëª…í™•
        """)
        
    with col_m2:
        st.markdown("#### 2. XGBoost (Main) ğŸ†")
        st.markdown("**ì—­í• **: ìµœì¢… ì„ íƒ ëª¨ë¸")
        st.markdown("""
        **ê°•ì **:
        - **Gradient Boosting**: ì´ì „ íŠ¸ë¦¬ì˜ ì˜¤ì°¨ë¥¼ ë‹¤ìŒ íŠ¸ë¦¬ê°€ í•™ìŠµ â†’ ì”ì—¬ ì˜¤ì°¨ ì§‘ìš”í•˜ê²Œ ê°ì†Œ
        - **ì •ê·œí™” (L1/L2 + Pruning)**: ê³¼ì í•© ë°©ì§€
        - **GPU ê°€ì†**: 110ë§Œ ê±´ ëŒ€ìš©ëŸ‰ ë°ì´í„° ë¹ ë¥¸ í•™ìŠµ
        - **ë¹„ì„ í˜• íŒ¨í„´**: Uìí˜• ê±´ì¶•ë…„ë„, ëŒ€ê°ì„  ì§€ë¦¬ íŒ¨í„´ íš¨ê³¼ì  í•™ìŠµ
        
        **ê²°ê³¼**:
        - 3ê°œ ëª¨ë¸ ì¤‘ **ì••ë„ì  ì„±ëŠ¥**
        - ì•™ìƒë¸”ë³´ë‹¤ ë‹¨ì¼ ëª¨ë¸ì´ ë” ìš°ìˆ˜
        """)

    with col_m3:
        st.markdown("#### 3. LightGBM (Sub)")
        st.markdown("**ì—­í• **: ì•™ìƒë¸” ì‹¤í—˜ìš©")
        st.markdown("""
        **ì¥ì **:
        - ëŒ€ìš©ëŸ‰ ë°ì´í„° í•™ìŠµ ì†ë„ ì••ë„ì 
        - ë¹ ë¥¸ ì‹¤í—˜ ë°˜ë³µ ê°€ëŠ¥
        
        **ì•™ìƒë¸” ì‹¤í—˜ ê²°ê³¼**:
        - RF + XGB + LGBM (1/3 ê°€ì¤‘í‰ê· )
        - **ì˜ˆìƒ**: ê° ëª¨ë¸ ì•½ì  ìƒì‡„
        - **ì‹¤ì œ**: ì˜¤íˆë ¤ ì„±ëŠ¥ ì €í•˜ âŒ
        - **ì›ì¸**: ëª¨ë¸ ê°„ ì˜ˆì¸¡ íŒ¨í„´ ì°¨ì´ë¡œ ë…¸ì´ì¦ˆ ì¦ê°€
        
        **ìµœì¢… ê²°ì •**: XGBoost Only
        """)
    
    st.warning("""ğŸ’¡ **êµí›ˆ**: ë§ì€ ëª¨ë¸ì„ ì„ëŠ”ë‹¤ê³  ë¬´ì¡°ê±´ ì¢‹ì€ ê²ƒì´ ì•„ë‹™ë‹ˆë‹¤. 
    **ê°€ì¥ ê°•ë ¥í•œ í•˜ë‚˜ë¥¼ ê·¹ëŒ€í™”**í•˜ëŠ” ê²ƒì´ ë” íš¨ê³¼ì ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.""")

    # -----------------------------------------------------------------
    # (New) Ensemble Performance Section (Displayed if available)
    # -----------------------------------------------------------------
    st.markdown("---")
    st.header("ğŸ† ì•™ìƒë¸” ëª¨ë¸ ì„±ëŠ¥ (RF + XGB + LGBM)")
    
    metric_path = 'codes/ensemble_metrics.json'
    if os.path.exists(metric_path):
        with open(metric_path, 'r', encoding='utf-8') as f:
            metrics = json.load(f)
        
        # 1. Summary Metrics
        st.info(f"ğŸ“… í•™ìŠµ ì™„ë£Œ ì‹œê°„: {metrics.get('timestamp', 'N/A')}")
        
        col1, col2 = st.columns(2)
        with col1:
            st.metric(label="ğŸ”¥ ìµœì¢… ì•™ìƒë¸” RMSE", value=f"{metrics['ensemble_rmse']:,.0f}")
        with col2:
            best_single_model = min(metrics['individual_rmse'], key=metrics['individual_rmse'].get)
            best_single_rmse = metrics['individual_rmse'][best_single_model]
            st.metric(label=f"ğŸ¥‡ ìµœê³  ë‹¨ì¼ ëª¨ë¸ ({best_single_model})", value=f"{best_single_rmse:,.0f}", 
                      delta=f"{metrics['ensemble_rmse'] - best_single_rmse:,.0f} (Improvement)")
        
        # 2. Individual Model Performance Chart
        st.subheader("ğŸ“Š ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ (RMSE ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)")
        rmses = metrics['individual_rmse']
        rmses['Ensemble (Weighted)'] = metrics['ensemble_rmse']
        
        rmse_df = pd.DataFrame(list(rmses.items()), columns=['Model', 'RMSE'])
        rmse_df = rmse_df.sort_values('RMSE', ascending=False)
        
        # Color highlight for Ensemble
        colors = ['#d3d3d3'] * len(rmse_df)
        rmse_df = rmse_df.reset_index(drop=True)
        try:
            ens_idx = rmse_df[rmse_df['Model'] == 'Ensemble (Weighted)'].index[0]
            colors[ens_idx] = '#ff4b4b' # Red for Ensemble
        except:
            pass
        
        fig_rmse, ax_rmse = plt.subplots(figsize=(10, 4))
        sns.barplot(data=rmse_df, x='RMSE', y='Model', palette=colors, ax=ax_rmse)
        ax_rmse.set_xlabel("Validation RMSE (Total Price)")
        for i, v in enumerate(rmse_df['RMSE']):
            ax_rmse.text(v, i, f" {v:,.0f}", va='center', fontweight='bold')
        st.pyplot(fig_rmse)

        # 3. Optimal Weights Chart
        st.subheader("âš–ï¸ ìµœì  ì•™ìƒë¸” ê°€ì¤‘ì¹˜ (Optimal Weights)")
        weights = metrics['optimal_weights']
        weights = {k: v for k, v in weights.items() if v > 0.001}
        
        if weights:
            fig_w, ax_w = plt.subplots(figsize=(6, 6))
            ax_w.pie(weights.values(), labels=weights.keys(), autopct='%1.1f%%', 
                     startangle=140, colors=['#66b3ff','#99ff99','#ffcc99'])
            ax_w.set_title("Model Contribution Weights")
            st.pyplot(fig_w)
        else:
            st.warning("ê°€ì¤‘ì¹˜ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # ---------------------------------------------------------
        # Experiment History Log
        # ---------------------------------------------------------
        st.markdown("---")
        st.subheader("ğŸ“‰ ì‹¤í—˜ ì´ë ¥ (Experiment Log)")
        st.markdown("ë‹¤ì–‘í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•©ì— ë”°ë¥¸ ì„±ëŠ¥ ë³€í™” ê¸°ë¡ì…ë‹ˆë‹¤.")
        
        exp_data = {
            "Model": ["XGBoost Only", "XGBoost Only", "XGBoost Only", "Ensemble (Mix)"],
            "Params": ["n_est=5000, lr=0.01", "n_est=5000, lr=0.02", "n_est=3000, lr=0.02", "RF+XGB+LGBM"],
            "RMSE (LB/Val)": ["ğŸš€ 15,114 (New Best!)", "15,469", "15,403", "âŒ 17,500+"],
            "Note": ["Transport Refinement + Clip", "Learning Rate 0.02 too high", "Good Baseline", "Overfitting"]
        }
        exp_df = pd.DataFrame(exp_data)
        st.table(exp_df)

        st.markdown("---")
        st.info("""
        **ğŸ’¡ ì°¸ê³ : ì´ ì ìˆ˜ëŠ” ì–´ë–»ê²Œ ë‚˜ì˜¤ë‚˜ìš”? (Validation RMSE)**
        
        ì´ ì ìˆ˜ëŠ” **'ëª¨ì˜ê³ ì‚¬ ì ìˆ˜'**ì…ë‹ˆë‹¤. 
        ë‹¨, ì‹œê³„ì—´ ë°ì´í„°ì˜ íŠ¹ì„±ì„ ë°˜ì˜í•˜ê¸° ìœ„í•´ ë¬´ì‘ìœ„ ë¶„í• ì´ ì•„ë‹Œ **'ë§ˆì§€ë§‰ 3ê°œì›” (Time Series Split)'** ë°ì´í„°ë¥¼ ê²€ì¦ ì…‹ìœ¼ë¡œ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.
        
        - **Validation Set (Last 3 Months)**: ê°€ì¥ ìµœê·¼ ê²½í–¥ì„ í…ŒìŠ¤íŠ¸ (ë¯¸ë˜ ì˜ˆì¸¡ ì‹œë®¬ë ˆì´ì…˜)
        - **Test Set**: ë¦¬ë”ë³´ë“œ ì œì¶œìš©
        
        ë”°ë¼ì„œ ì´ ì ìˆ˜ê°€ ì˜ ë‚˜ì˜¤ë©´, ì‹¤ì œ ë¦¬ë”ë³´ë“œ(ë¯¸ë˜ ë°ì´í„°) ì„±ì ë„ ì¢‹ì„ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.
        """)
        
    else:
        st.warning("âš ï¸ ì•™ìƒë¸” í•™ìŠµ ê²°ê³¼ íŒŒì¼ ('ensemble_metrics.json')ì´ ì—†ìŠµë‹ˆë‹¤. train_ensemble.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.")

# -------------------------------------------------------------------------------------------
# [6] Final Results
# -------------------------------------------------------------------------------------------
with tab5:
    st.header("5. ìµœì¢… ê²°ê³¼ ë° ì œì–¸")
    
    st.balloons()
    
    col1, col2, col3 = st.columns(3)
    col1.metric("Final Leaderboard", "15,114", "Best Score")
    col2.metric("Validation RMSE", "12,200", "Last 3 Months")
    col3.metric("Improvement", "1,513 â–¼", "from Baseline(16,627)")
    
    st.divider()
    st.subheader("ğŸ“Š ëª¨ë¸ì´ ì£¼ëª©í•œ í•µì‹¬ ë³€ìˆ˜ (Top Features)")
    try:
        fi_df = pd.read_csv('codes/feature_importance.csv')
        fig_fi = plt.figure(figsize=(10, 5))
        sns.barplot(data=fi_df.head(15), x='importance', y='feature', palette='viridis')
        plt.title("Top 15 Feature Importance")
        st.pyplot(fig_fi)
    except:
        st.info("íŠ¹ì„± ì¤‘ìš”ë„ ë°ì´í„°ê°€ ì•„ì§ ì—†ìŠµë‹ˆë‹¤. (í•™ìŠµ í›„ ìƒì„±ë¨)")
    
    st.markdown("### âœ… ê²°ë¡  (Conclusion)")
    st.markdown("""
    1. **Unit Price Target**: ë‹¨ìˆœ ê°€ê²© ì˜ˆì¸¡ë³´ë‹¤ í‰ë‹¹ ë‹¨ê°€ ì˜ˆì¸¡ì´ í›¨ì”¬ íš¨ê³¼ì ì´ì—ˆìŠµë‹ˆë‹¤.
    2. **Refined Geo Features**: ë‹¨ìˆœ ê±°ë¦¬ë¿ë§Œ ì•„ë‹ˆë¼ '300m/500m/800m' ë“± ì„¸ë¶„í™”ëœ ë°€ë„ í”¼ì²˜ê°€ ì„±ëŠ¥ í–¥ìƒì˜ í•µì‹¬ì´ì—ˆìŠµë‹ˆë‹¤.
    3. **Future Work**: ì´ì œ **Bayesian Optimization (Optuna)**ë¥¼ í™œìš©í•´ XGBoostì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì •ë°€í•˜ê²Œ íŠœë‹í•˜ë©´ RMSE 14,000ì ëŒ€ ì§„ì…ë„ ê°€ëŠ¥í•  ê²ƒì…ë‹ˆë‹¤.
    """)

# Footer
st.sidebar.markdown("---")
