# ==================================================================================
# ì„œìš¸ì‹œ ì•„íŒŒíŠ¸ ì‹¤ê±°ë˜ê°€ ì˜ˆì¸¡ ëª¨ë¸ (XGBoost Only Version)
# ==================================================================================
# 
# [í”„ë¡œì íŠ¸ ëª©í‘œ]
# - ì„œìš¸ì‹œ ì•„íŒŒíŠ¸ ì‹¤ê±°ë˜ê°€ë¥¼ ì˜ˆì¸¡í•˜ì—¬ RMSE(í‰ê·  ì œê³±ê·¼ ì˜¤ì°¨)ë¥¼ ìµœì†Œí™”
# - Baseline ëª¨ë¸(Random Forest) ëŒ€ë¹„ ì„±ëŠ¥ ê°œì„ 
#
# [í•µì‹¬ ì „ëµ]
# 1. **Target Transformation**: 'ê±°ë˜ê¸ˆì•¡' ëŒ€ì‹  'í‰ë‹¹ê°€(Unit Price)'ë¥¼ ì˜ˆì¸¡
#    â†’ ë©´ì ì— ë”°ë¥¸ ê°€ê²© í¸ì°¨ë¥¼ ì •ê·œí™”í•˜ì—¬ ëª¨ë¸ í•™ìŠµ íš¨ìœ¨ í–¥ìƒ
#
# 2. **Feature Engineering**: 
#    - ì§€ë¦¬ì  íŠ¹ì„±: K-Means Clustering (1000ê°œ í´ëŸ¬ìŠ¤í„°)
#    - êµí†µ ë°€ë„: ë²„ìŠ¤/ì§€í•˜ì²  ë°˜ê²½ë³„ ê°œìˆ˜ (300m, 500m, 800m, 1200m)
#    - íšŒì „ ì¢Œí‘œ: 45ë„ íšŒì „ìœ¼ë¡œ ì„œìš¸ì˜ ëŒ€ê°ì„  ì§€ë¦¬ íŒ¨í„´ í•™ìŠµ
#
# 3. **Model Selection**: 
#    - ì´ˆê¸°: Random Forest (Baseline)
#    - ì‹¤í—˜: RF + XGBoost + LightGBM ì•™ìƒë¸” (ì‹¤íŒ¨ - ì„±ëŠ¥ ì €í•˜)
#    - ìµœì¢…: **XGBoost ë‹¨ì¼ ëª¨ë¸** (ê°€ì¥ ìš°ìˆ˜í•œ ì„±ëŠ¥)
#
# 4. **Validation Strategy**: 
#    - Time Series Split (ìµœê·¼ 3ê°œì›” Cutoff)
#    - Look-ahead Bias ë°©ì§€ë¡œ ë¦¬ë”ë³´ë“œ ì ìˆ˜ì™€ CV ì ìˆ˜ ì¼ì¹˜
#
# [ìµœì¢… ì„±ê³¼]
# - Baseline RMSE: 16,627 â†’ Final RMSE: 15,114 (ì•½ 9% ê°œì„ )
# ==================================================================================

# ==================================================================================
# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
# ==================================================================================
import pandas as pd              # ë°ì´í„°í”„ë ˆì„ ì²˜ë¦¬
import numpy as np               # ìˆ˜ì¹˜ ì—°ì‚°
from sklearn.ensemble import RandomForestRegressor, VotingRegressor  # ì•™ìƒë¸” ëª¨ë¸ (í˜„ì¬ ë¯¸ì‚¬ìš©)
from xgboost import XGBRegressor  # XGBoost ëª¨ë¸ (ë©”ì¸ ëª¨ë¸)
from lightgbm import LGBMRegressor  # LightGBM ëª¨ë¸ (í˜„ì¬ ë¯¸ì‚¬ìš©)
from sklearn.cluster import KMeans, MiniBatchKMeans  # K-Means í´ëŸ¬ìŠ¤í„°ë§ (ì§€ë¦¬ì  ê·¸ë£¹í™”)
from sklearn.preprocessing import LabelEncoder, StandardScaler  # ì¸ì½”ë”© ë° ìŠ¤ì¼€ì¼ë§
from sklearn.metrics import mean_squared_error  # RMSE ê³„ì‚°
import os  # íŒŒì¼ ê²½ë¡œ ì²˜ë¦¬
import json  # ë©”íƒ€ë°ì´í„° ì €ì¥

# ==================================================================================
# 1. ë°ì´í„° ë¡œë“œ í•¨ìˆ˜
# ==================================================================================
def load_data():
    """
    í•™ìŠµ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    
    Returns:
        train (DataFrame): í•™ìŠµ ë°ì´í„° (ì•½ 110ë§Œ ê±´)
        test (DataFrame): í…ŒìŠ¤íŠ¸ ë°ì´í„° (ì•½ 9ì²œ ê±´)
    """
    print("ğŸ“‚ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")
    train = pd.read_csv('../data/train.csv', low_memory=False)  # low_memory=False: ë°ì´í„° íƒ€ì… ì¶”ë¡  ì •í™•ë„ í–¥ìƒ
    test = pd.read_csv('../data/test.csv', low_memory=False)
    return train, test

# ==================================================================================
# 2. ì»¬ëŸ¼ëª… ë³€í™˜ í•¨ìˆ˜
# ==================================================================================
def rename_columns(df):
    """
    í•œê¸€ ì»¬ëŸ¼ëª…ì„ ë‹¨ìˆœí™”í•˜ì—¬ ì½”ë“œ ê°€ë…ì„±ì„ ë†’ì…ë‹ˆë‹¤.
    
    ì£¼ìš” ë³€í™˜:
    - 'ì „ìš©ë©´ì (ã¡)' â†’ 'ì „ìš©ë©´ì '
    - 'ì¢Œí‘œX' â†’ 'ê²½ë„' (Longitude)
    - 'ì¢Œí‘œY' â†’ 'ìœ„ë„' (Latitude)
    - 'target' â†’ 'ê±°ë˜ê¸ˆì•¡' (ì‹¤ì œ ê±°ë˜ ê¸ˆì•¡)
    
    Args:
        df (DataFrame): ë³€í™˜í•  ë°ì´í„°í”„ë ˆì„
    
    Returns:
        DataFrame: ì»¬ëŸ¼ëª…ì´ ë³€í™˜ëœ ë°ì´í„°í”„ë ˆì„
    """
    cols_mapping = {
        'ì „ìš©ë©´ì (ã¡)': 'ì „ìš©ë©´ì ',
        'ì¢Œí‘œX': 'ê²½ë„',  # longitude
        'ì¢Œí‘œY': 'ìœ„ë„',  # latitude
        'target': 'ê±°ë˜ê¸ˆì•¡'  # ì˜ˆì¸¡ ëŒ€ìƒ (ì‹¤ê±°ë˜ê°€)
    }
    # ì»¬ëŸ¼ëª…ì´ ì´ë¯¸ ë³€í™˜ë˜ì–´ ìˆì„ ìˆ˜ë„ ìˆìœ¼ë¯€ë¡œ, ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ rename
    df = df.rename(columns=cols_mapping)
    return df

# ==================================================================================
# 3. Label Encoding í•¨ìˆ˜ (Train ê¸°ì¤€)
# ==================================================================================
def label_encode_train_test(train_s, test_s):
    """
    ë²”ì£¼í˜• ë³€ìˆ˜ë¥¼ ìˆ«ìë¡œ ë³€í™˜í•©ë‹ˆë‹¤ (Label Encoding).
    
    [ì¤‘ìš”] Train ë°ì´í„°ì˜ ê³ ìœ ê°’ë§Œ ì‚¬ìš©í•˜ì—¬ ë§¤í•‘ì„ ìƒì„±í•©ë‹ˆë‹¤.
    Test ë°ì´í„°ì— Trainì— ì—†ëŠ” ê°’ì´ ìˆìœ¼ë©´ 'Unknown'ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    
    ì˜ˆì‹œ:
    - Train: ['ê°•ë‚¨êµ¬', 'ì„œì´ˆêµ¬', 'ì†¡íŒŒêµ¬'] â†’ {ê°•ë‚¨êµ¬: 0, ì„œì´ˆêµ¬: 1, ì†¡íŒŒêµ¬: 2}
    - Test: ['ê°•ë‚¨êµ¬', 'ë§ˆí¬êµ¬'] â†’ [0, 3]  # 'ë§ˆí¬êµ¬'ëŠ” Unknown(3)ìœ¼ë¡œ ì²˜ë¦¬
    
    Args:
        train_s (Series): Train ë°ì´í„°ì˜ ë²”ì£¼í˜• ì»¬ëŸ¼
        test_s (Series): Test ë°ì´í„°ì˜ ë²”ì£¼í˜• ì»¬ëŸ¼
    
    Returns:
        tuple: (train_encoded, test_encoded) - ì¸ì½”ë”©ëœ Series ìŒ
    """
    # ëª¨ë“  ê°’ì„ ë¬¸ìì—´ë¡œ ë³€í™˜ (ê²°ì¸¡ì¹˜ ì²˜ë¦¬ í¬í•¨)
    train_s = train_s.astype(str)
    test_s = test_s.astype(str)
    
    # Trainì˜ ê³ ìœ ê°’ìœ¼ë¡œ ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ ìƒì„±
    uniq = pd.Index(train_s.unique())
    mapping = {k: i for i, k in enumerate(uniq)}  # {ê°’: ì¸ë±ìŠ¤}
    unk = len(mapping)  # Unknownì€ ë§ˆì§€ë§‰ ë²ˆí˜¸ ë¶€ì—¬ (ì˜ˆ: 3ê°œ ê³ ìœ ê°’ì´ë©´ Unknown=3)
    
    # Trainê³¼ Testì— ë§¤í•‘ ì ìš© (Testì˜ Unknown ê°’ì€ unkë¡œ ì±„ì›€)
    return train_s.map(mapping).fillna(unk).astype(int), test_s.map(mapping).fillna(unk).astype(int)

# ==================================================================================
# 4. ë°ì´í„° ì „ì²˜ë¦¬ ë° íŠ¹ì„± ê³µí•™ ë©”ì¸ í•¨ìˆ˜
# ==================================================================================
def preprocess_data(train, test):
    """
    ë°ì´í„° ì „ì²˜ë¦¬ ë° íŠ¹ì„± ê³µí•™(Feature Engineering)ì„ ìˆ˜í–‰í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜ì…ë‹ˆë‹¤.
    
    [ì£¼ìš” ì²˜ë¦¬ ë‹¨ê³„]
    1. ì»¬ëŸ¼ëª… ë³€í™˜ ë° ë¶ˆí•„ìš”í•œ ë³€ìˆ˜ ì œê±° (27ê°œ ë…¸ì´ì¦ˆ ë³€ìˆ˜)
    2. Target Transformation: 'ê±°ë˜ê¸ˆì•¡' â†’ 'í‰ë‹¹ê°€' (ë©´ì  ì •ê·œí™”)
    3. ì£¼ì†Œ ì •ë³´ ë¶„í• : 'ì‹œêµ°êµ¬' â†’ 'ì‹œ', 'êµ¬', 'ë™'
    4. ë‚ ì§œ íŠ¹ì„±: ê±°ë˜ë…„ë„, ê±°ë˜ì›”, ì—°ì‹ ê³„ì‚°
    5. êµí†µ íŠ¹ì„±: ë²„ìŠ¤/ì§€í•˜ì²  ê±°ë¦¬ ë° ë°€ë„ (BallTree ì‚¬ìš©)
    6. ì§€ë¦¬ì  í´ëŸ¬ìŠ¤í„°ë§: K-Means (1000ê°œ í´ëŸ¬ìŠ¤í„°)
    7. íšŒì „ ì¢Œí‘œ: 45ë„ íšŒì „ìœ¼ë¡œ ëŒ€ê°ì„  íŒ¨í„´ í•™ìŠµ
    8. Label Encoding: ë²”ì£¼í˜• ë³€ìˆ˜ â†’ ìˆ«ì
    9. ê²°ì¸¡ì¹˜ ì²˜ë¦¬: Train Medianìœ¼ë¡œ ì±„ìš°ê¸°
    
    Args:
        train (DataFrame): í•™ìŠµ ë°ì´í„°
        test (DataFrame): í…ŒìŠ¤íŠ¸ ë°ì´í„°
    
    Returns:
        tuple: (X_train, y_train, X_test) - ì „ì²˜ë¦¬ëœ íŠ¹ì„±ê³¼ íƒ€ê²Ÿ
    """
    print("âš™ï¸ ë°ì´í„° ì „ì²˜ë¦¬ ì§„í–‰ ì¤‘...")
    
    # ----------------------------------------------------------------
    # Step 1: ì»¬ëŸ¼ëª… ë³€í™˜
    # ----------------------------------------------------------------
    train = rename_columns(train)
    test = rename_columns(test)

    # ----------------------------------------------------------------
    # Step 2: ë¶ˆí•„ìš”í•œ ë³€ìˆ˜ ì œê±° (Noise Reduction)
    # ----------------------------------------------------------------
    # [ì„¤ëª…] ì˜ˆì¸¡ì— ë„ì›€ì´ ë˜ì§€ ì•ŠëŠ” 27ê°œì˜ ë…¸ì´ì¦ˆ ë³€ìˆ˜ë¥¼ ì œê±°í•©ë‹ˆë‹¤.
    # - ê´€ë¦¬ë¹„, ì „í™”ë²ˆí˜¸, í™ˆí˜ì´ì§€ ë“± ì˜ˆì¸¡ê³¼ ë¬´ê´€í•œ ì •ë³´
    # - ì„¸ëŒ€í˜„í™© ì„¸ë¶€ ë¶„ë¥˜ (ì´ë¯¸ 'ì „ì²´ì„¸ëŒ€ìˆ˜'ë¡œ ìš”ì•½ë¨)
    # - ë‚ ì§œ ì •ë³´ (ë“±ë¡ì¼ì, ìˆ˜ì •ì¼ì ë“±)
    cols_to_drop = [
        'í•´ì œì‚¬ìœ ë°œìƒì¼', 'ë‹¨ì§€ì†Œê°œê¸°ì¡´clob', 'k-ê´€ë¦¬ë¹„ë¶€ê³¼ë©´ì ',
        'k-ì „ìš©ë©´ì ë³„ì„¸ëŒ€í˜„í™©(60ã¡ì´í•˜)', 'k-ì „ìš©ë©´ì ë³„ì„¸ëŒ€í˜„í™©(60ã¡~85ã¡ì´í•˜)',
        'k-85ã¡~135ã¡ì´í•˜', 'k-135ã¡ì´ˆê³¼', 'ê±´ì¶•ë©´ì ', 'K-ì „í™”ë²ˆí˜¸', 'K-íŒ©ìŠ¤ë²ˆí˜¸', 'k-ì„¸ëŒ€íƒ€ì…(ë¶„ì–‘í˜•íƒœ)',
        'k-ê´€ë¦¬ë°©ì‹','k-ë³µë„ìœ í˜•','k-ë‚œë°©ë°©ì‹','k-ì‚¬ìš©ê²€ì‚¬ì¼-ì‚¬ìš©ìŠ¹ì¸ì¼','k-í™ˆí˜ì´ì§€','k-ë“±ë¡ì¼ì','k-ìˆ˜ì •ì¼ì',
        'ê³ ìš©ë³´í—˜ê´€ë¦¬ë²ˆí˜¸','ê²½ë¹„ë¹„ê´€ë¦¬í˜•íƒœ','ì„¸ëŒ€ì „ê¸°ê³„ì•½ë°©ë²•','ì²­ì†Œë¹„ê´€ë¦¬í˜•íƒœ','ê¸°íƒ€/ì˜ë¬´/ì„ëŒ€/ì„ì˜=1/2/3/4','ë‹¨ì§€ìŠ¹ì¸ì¼','ì‚¬ìš©í—ˆê°€ì—¬ë¶€','ê´€ë¦¬ë¹„ ì—…ë¡œë“œ','ë‹¨ì§€ì‹ ì²­ì¼',
    ]
    train = train.drop(columns=cols_to_drop, errors='ignore')  # errors='ignore': ì—†ëŠ” ì»¬ëŸ¼ì€ ë¬´ì‹œ
    test = test.drop(columns=cols_to_drop, errors='ignore')
    print(f"âœ… Dropped {len(cols_to_drop)} noise features")

    # ë¶ˆí•„ìš”í•œ ì¸ë±ìŠ¤ ì»¬ëŸ¼ ì œê±° (CSV ì €ì¥ ì‹œ ìƒì„±ëœ ì¸ë±ìŠ¤)
    if 'Unnamed: 0' in train.columns:
        train = train.drop(columns=['Unnamed: 0'])
    if 'Unnamed: 0' in test.columns:
        test = test.drop(columns=['Unnamed: 0'])

    target_col = 'ê±°ë˜ê¸ˆì•¡'  # ì˜ˆì¸¡ ëŒ€ìƒ: ì‹¤ê±°ë˜ê°€ (ë§Œì› ë‹¨ìœ„)
    
    # ----------------------------------------------------------------
    # Step 3: [í•µì‹¬] Target Transformation - Unit Price (í‰ë‹¹ê°€) ìƒì„±
    # ----------------------------------------------------------------
    # [ì„¤ëª…] 'ê±°ë˜ê¸ˆì•¡'ì„ ì§ì ‘ ì˜ˆì¸¡í•˜ë©´ ë©´ì ì— ë”°ë¥¸ í¸ì°¨ê°€ ë„ˆë¬´ í½ë‹ˆë‹¤.
    # ì˜ˆ: 10í‰ ì•„íŒŒíŠ¸ 2ì–µ vs 30í‰ ì•„íŒŒíŠ¸ 6ì–µ â†’ ëª¨ë¸ì´ ë©´ì ë§Œ í•™ìŠµ
    # 
    # [í•´ê²°ì±…] 'í‰ë‹¹ê°€(Unit Price)'ë¥¼ ì˜ˆì¸¡í•˜ì—¬ ë©´ì  íš¨ê³¼ë¥¼ ì •ê·œí™”í•©ë‹ˆë‹¤.
    # - í‰ë‹¹ê°€ = ê±°ë˜ê¸ˆì•¡ / ì „ìš©ë©´ì 
    # - ì˜ˆì¸¡ ì‹œ: í‰ë‹¹ê°€ * ì „ìš©ë©´ì  = ê±°ë˜ê¸ˆì•¡ (ë³µì›)
    # 
    # [íš¨ê³¼] Baseline ëŒ€ë¹„ ì•½ 448ì  ê°œì„  (16,627 â†’ 16,179)
    train['í‰ë‹¹ê°€'] = train[target_col] / train['ì „ìš©ë©´ì ']
    
    # ----------------------------------------------------------------
    # Step 4: Train/Test ë°ì´í„° ë³‘í•© (ì „ì²˜ë¦¬ ì¼ê´„ ì ìš©)
    # ----------------------------------------------------------------
    # [ì´ìœ ] Feature Engineeringì„ Train/Testì— ë™ì¼í•˜ê²Œ ì ìš©í•˜ê¸° ìœ„í•¨
    # ë‚˜ì¤‘ì— 'is_train' ì»¬ëŸ¼ìœ¼ë¡œ ë‹¤ì‹œ ë¶„ë¦¬í•©ë‹ˆë‹¤.
    train['is_train'] = 1  # í•™ìŠµ ë°ì´í„° ì‹ë³„ì
    test['is_train'] = 0   # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì‹ë³„ì
    test[target_col] = np.nan  # Testì—ëŠ” ì •ë‹µì´ ì—†ìŒ
    test['í‰ë‹¹ê°€'] = np.nan
    
    data = pd.concat([train, test], axis=0, ignore_index=True)  # ë³‘í•© (ì•½ 110ë§Œ + 9ì²œ = 119ë§Œ ê±´)
    
    # ----------------------------------------------------------------
    # Metadata ì´ˆê¸°í™” (Streamlit ì•±ì—ì„œ ì‚¬ìš©)
    # ----------------------------------------------------------------
    meta = {
        'dropped_features': cols_to_drop,  # ì œê±°ëœ ë³€ìˆ˜ ëª©ë¡
        'imputation': {},  # ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì •ë³´
        'encoding': {},  # ì¸ì½”ë”© ì •ë³´
        'address_example': {}  # ì£¼ì†Œ ë¶„í•  ì˜ˆì‹œ
    }

    # ----------------------------------------------------------------
    # Step 5: ì£¼ì†Œ ì •ë³´ ë¶„í•  (ì‹œ/êµ¬/ë™)
    # ----------------------------------------------------------------
    # [ì„¤ëª…] 'ì‹œêµ°êµ¬' ì»¬ëŸ¼ì„ ê³µë°± ê¸°ì¤€ìœ¼ë¡œ ë¶„í• í•˜ì—¬ í–‰ì •êµ¬ì—­ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    # ì˜ˆ: "ì„œìš¸íŠ¹ë³„ì‹œ ê°•ë‚¨êµ¬ ì—­ì‚¼ë™" â†’ ì‹œ="ì„œìš¸íŠ¹ë³„ì‹œ", êµ¬="ê°•ë‚¨êµ¬", ë™="ì—­ì‚¼ë™"
    # 
    # [ì¤‘ìš”ì„±] 'êµ¬' ì •ë³´ëŠ” Feature Importance 1ìœ„ (ê°•ë‚¨ í”„ë¦¬ë¯¸ì—„ ë°˜ì˜)
    print("ğŸ“ ì£¼ì†Œ ì •ë³´ ì²˜ë¦¬ ì¤‘...")
    data['ì‹œêµ°êµ¬'] = data['ì‹œêµ°êµ¬'].fillna("Unknown Unknown Unknown")  # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    sigungu_split = data['ì‹œêµ°êµ¬'].str.split(' ', expand=True)  # ê³µë°±ìœ¼ë¡œ ë¶„í• 
    
    # ë¶„í•  ê²°ê³¼ì— ë”°ë¼ ì‹œ/êµ¬/ë™ í• ë‹¹
    if sigungu_split.shape[1] >= 3:  # ì •ìƒì ìœ¼ë¡œ 3ê°œ ì´ìƒ ë¶„í• ëœ ê²½ìš°
        data['ì‹œ'] = sigungu_split[0]  # ì˜ˆ: "ì„œìš¸íŠ¹ë³„ì‹œ"
        data['êµ¬'] = sigungu_split[1]  # ì˜ˆ: "ê°•ë‚¨êµ¬" (ê°€ì¥ ì¤‘ìš”!)
        data['ë™'] = sigungu_split[2]  # ì˜ˆ: "ì—­ì‚¼ë™"
    else:  # ë¶„í•  ì‹¤íŒ¨ ì‹œ Unknown ì²˜ë¦¬
        data['ì‹œ'] = sigungu_split[0]
        data['êµ¬'] = 'Unknown'
        data['ë™'] = 'Unknown'
    
    # ì˜ˆì‹œ ì €ì¥ (Streamlit ì•±ì—ì„œ í‘œì‹œìš©)
    split_ex = f"{data['ì‹œêµ°êµ¬'].iloc[0]} â†’ {data['ì‹œ'].iloc[0]}, {data['êµ¬'].iloc[0]}, {data['ë™'].iloc[0]}"
    meta['address_example'] = split_ex
            
    # ----------------------------------------------------------------
    # Step 6: ë‚ ì§œ ë° ì—°ì‹ ì •ë³´ íŒŒìƒ
    # ----------------------------------------------------------------
    # [ì„¤ëª…] ê³„ì•½ë…„ì›”ì—ì„œ ë…„ë„/ì›”ì„ ì¶”ì¶œí•˜ê³ , ê±´ì¶•ë…„ë„ì™€ì˜ ì°¨ì´ë¡œ ì—°ì‹ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
    # - ê±°ë˜ë…„ë„/ì›”: ì‹œì¥ ì‚¬ì´í´ ë°˜ì˜ (2022ë…„ ê¸ˆë¦¬ ì¸ìƒ ë“±)
    # - ì—°ì‹: ì‹ ì¶• í”„ë¦¬ë¯¸ì—„ ë°˜ì˜ (Uìí˜• íŒ¨í„´: ì¬ê±´ì¶• ê¸°ëŒ€ + ì‹ ì¶•)
    data['ê³„ì•½ë…„ì›”'] = data['ê³„ì•½ë…„ì›”'].astype(str)  # ë¬¸ìì—´ ë³€í™˜
    data['ê±°ë˜ë…„ë„'] = data['ê³„ì•½ë…„ì›”'].str[:4].astype(int)  # ì• 4ìë¦¬: ë…„ë„
    data['ê±°ë˜ì›”'] = data['ê³„ì•½ë…„ì›”'].str[4:].astype(int)  # ë’¤ 2ìë¦¬: ì›”
    
    # ì•„íŒŒíŠ¸ ì—°ì‹ (Age) ê³„ì‚°: ê±°ë˜ë…„ë„ - ê±´ì¶•ë…„ë„
    # ì˜ˆ: 2023ë…„ ê±°ë˜, 2000ë…„ ê±´ì¶• â†’ ì—°ì‹ 23ë…„
    data['ì—°ì‹'] = data['ê±°ë˜ë…„ë„'] - data['ê±´ì¶•ë…„ë„']

    # 4. [í•µì‹¬] ì§€ë¦¬ì  íŠ¹ì„± ê°•í™” (Enhanced Geo Features)
    print("ğŸšŒ ì§€ë¦¬ ì •ë³´ ê°•í™” ì¤‘ (í´ëŸ¬ìŠ¤í„°ë§ & êµí†µ í”¼ì²˜)...")
    
    # ----------------------------------------------------------------
    # 4-0. êµí†µ í¸ì˜ì„± íŠ¹ì„± ì¶”ê°€ (Bus & Subway) - Advanced
    # ----------------------------------------------------------------
    try:
        from sklearn.neighbors import BallTree
        print("  êµí†µ ë°ì´í„° ë¡œë“œ ì¤‘ (Bus & Subway)...")
        
        bus = pd.read_csv('../data/bus_feature.csv')
        sub = pd.read_csv('../data/subway_feature.csv')
        
        # ê²°ì¸¡ ì¢Œí‘œ ì œê±°
        bus = bus.dropna(subset=['Xì¢Œí‘œ', 'Yì¢Œí‘œ'])
        sub = sub.dropna(subset=['ê²½ë„', 'ìœ„ë„'])
        
        # ì¢Œí‘œ ë°ì´í„° ì¤€ë¹„ (Radians for Haversine)
        # Bus: Yì¢Œí‘œ=Lat, Xì¢Œí‘œ=Lon
        bus_rad = np.radians(bus[['Yì¢Œí‘œ', 'Xì¢Œí‘œ']].values)
        # Sub: ìœ„ë„=Lat, ê²½ë„=Lon
        sub_rad = np.radians(sub[['ìœ„ë„', 'ê²½ë„']].values)
        
        # Data: ìœ„ë„=Lat, ê²½ë„=Lon
        data_coords = data[['ìœ„ë„', 'ê²½ë„']].fillna(0)
        data_rad = np.radians(data_coords.values)
        
        # -------------------------------------------------------
        # 7-1. ìµœë‹¨ ê±°ë¦¬ (Nearest Distance)
        # -------------------------------------------------------
        # [ì„¤ëª…] ê°€ì¥ ê°€ê¹Œìš´ ë²„ìŠ¤ì •ë¥˜ì¥/ì§€í•˜ì² ì—­ê¹Œì§€ì˜ ê±°ë¦¬ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        # - BallTree.query(k=1): ê°€ì¥ ê°€ê¹Œìš´ 1ê°œ ì§€ì  ê²€ìƒ‰
        # - ë°˜í™˜ê°’: ë¼ë””ì•ˆ ë‹¨ìœ„ ê±°ë¦¬ â†’ ë¯¸í„° ë³€í™˜ (* 6371000)
        # - Clipping: ì´ìƒì¹˜ ì œê±° (ë²„ìŠ¤ 2km, ì§€í•˜ì²  5km ìƒí•œ)
        print("  - ìµœë‹¨ ê±°ë¦¬ ê³„ì‚° ì¤‘...")
        
        # 7-1-1. Bus ìµœë‹¨ ê±°ë¦¬
        tree_bus = BallTree(bus_rad, metric='haversine')  # BallTree ìƒì„± (ë²„ìŠ¤)
        dist_bus, _ = tree_bus.query(data_rad, k=1)  # k=1: ê°€ì¥ ê°€ê¹Œìš´ 1ê°œ
        data['dist_to_bus'] = (dist_bus[:, 0] * 6371000).astype(np.float32).clip(0, 2000)  # 0~2km ì œí•œ
        
        # 7-1-2. Subway ìµœë‹¨ ê±°ë¦¬
        tree_sub = BallTree(sub_rad, metric='haversine')  # BallTree ìƒì„± (ì§€í•˜ì² )
        dist_sub, _ = tree_sub.query(data_rad, k=1)
        data['dist_to_subway'] = (dist_sub[:, 0] * 6371000).astype(np.float32).clip(0, 5000)  # 0~5km ì œí•œ

        # -------------------------------------------------------
        # 7-2. ë°˜ê²½ ë‚´ ê°œìˆ˜ (Count within Radius) - ë°€ë„ ì¸¡ì •
        # -------------------------------------------------------
        # [ì„¤ëª…] ì—¬ëŸ¬ ë°˜ê²½(300m, 500m, 800m, 1200m) ë‚´ì— ëª‡ ê°œì˜ ì •ë¥˜ì¥/ì—­ì´ ìˆëŠ”ì§€ ê³„ì‚°í•©ë‹ˆë‹¤.
        # - ë°˜ê²½ì´ ì‘ì„ìˆ˜ë¡: ë” ì„¸ë°€í•œ ì—­ì„¸ê¶Œ íŒë‹¨
        # - ë°˜ê²½ì´ í´ìˆ˜ë¡: ë” ë„“ì€ êµí†µ í¸ì˜ì„± ë°˜ì˜
        # 
        # [íš¨ê³¼] ë‹¤ì–‘í•œ ë°˜ê²½ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì´ ë¹„ì„ í˜• íŒ¨í„´ì„ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        print("  - ë°˜ê²½ ë‚´ ê°œìˆ˜ ê³„ì‚° ì¤‘ (Bus: 300/500/800, Subway: 300/500/800/1200)...")
        
        # 7-2-1. Bus ë°€ë„ (300m, 500m, 800m)
        for r in [300, 500, 800]:  # ë°˜ê²½ (ë¯¸í„°)
            radius_rad = r / 6371000  # ë¯¸í„° â†’ ë¼ë””ì•ˆ ë³€í™˜
            count = tree_bus.query_radius(data_rad, r=radius_rad, count_only=True)  # ë°˜ê²½ ë‚´ ê°œìˆ˜
            data[f'bus_cnt_{r}'] = count  # ì˜ˆ: bus_cnt_300, bus_cnt_500, bus_cnt_800
        
        # 7-2-2. Subway ë°€ë„ (300m, 500m, 800m, 1200m)
        for r in [300, 500, 800, 1200]:  # ì§€í•˜ì² ì€ 1200mê¹Œì§€ í™•ì¥
            radius_rad = r / 6371000
            count = tree_sub.query_radius(data_rad, r=radius_rad, count_only=True)
            data[f'sub_cnt_{r}'] = count  # ì˜ˆ: sub_cnt_300, sub_cnt_500, sub_cnt_800, sub_cnt_1200
        
        # -------------------------------------------------------
        # 7-3. ì§€í•˜ì²  ì ‘ê·¼ì„± ì ìˆ˜ (Weighted Subway Score)
        # -------------------------------------------------------
        # [ì„¤ëª…] ëª¨ë“  ì§€í•˜ì² ì´ ë™ì¼í•œ ê°€ì¹˜ë¥¼ ê°€ì§€ì§€ ì•ŠìŠµë‹ˆë‹¤!
        # - 2í˜¸ì„ , 9í˜¸ì„ , ì‹ ë¶„ë‹¹ì„ : ê°€ì¥ ì¤‘ìš” (2.0 ê°€ì¤‘ì¹˜)
        # - 3í˜¸ì„ , 5í˜¸ì„ , 7í˜¸ì„ : ì¤‘ê°„ (1.5 ê°€ì¤‘ì¹˜)
        # - 1í˜¸ì„ , 4í˜¸ì„ , 6í˜¸ì„ , 8í˜¸ì„ : ê¸°ë³¸ (1.2 ê°€ì¤‘ì¹˜)
        # - ê¸°íƒ€ í˜¸ì„ : 1.0 ê°€ì¤‘ì¹˜
        # 
        # [ì´ìœ ] ê°•ë‚¨ ì§€ì—­ì„ ê´€í†µí•˜ëŠ” í˜¸ì„ ì¼ìˆ˜ë¡ í”„ë¦¬ë¯¸ì—„ì´ ë†’ìŠµë‹ˆë‹¤.
        print("  - ì§€í•˜ì²  ì ‘ê·¼ì„± ì ìˆ˜ ê³„ì‚° ì¤‘ (í˜¸ì„ ë³„ ê°€ì¤‘ì¹˜ ì ìš©)...")
        
        def get_line_weight(line_name):
            """
            ì§€í•˜ì²  í˜¸ì„ ë³„ ê°€ì¤‘ì¹˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
            
            ê°€ì¤‘ì¹˜ ê¸°ì¤€:
            - 2.0: ê°•ë‚¨ í•µì‹¬ í˜¸ì„  (2í˜¸ì„ , 9í˜¸ì„ , ì‹ ë¶„ë‹¹ì„ )
            - 1.5: ì£¼ìš” í˜¸ì„  (3í˜¸ì„ , 5í˜¸ì„ , 7í˜¸ì„ )
            - 1.2: ê¸°ë³¸ í˜¸ì„  (1í˜¸ì„ , 4í˜¸ì„ , 6í˜¸ì„ , 8í˜¸ì„ )
            - 1.0: ê¸°íƒ€
            """
            if any(l in line_name for l in ['2í˜¸ì„ ', '9í˜¸ì„ ', 'ì‹ ë¶„ë‹¹', 'ë¶„ë‹¹ì„ ']):
                return 2.0  # ê°€ì¥ ì¤‘ìš”í•œ í˜¸ì„ 
            elif any(l in line_name for l in ['3í˜¸ì„ ', '7í˜¸ì„ ', '5í˜¸ì„ ']):
                return 1.5  # ì£¼ìš” í˜¸ì„ 
            elif any(l in line_name for l in ['1í˜¸ì„ ', '4í˜¸ì„ ', '6í˜¸ì„ ', '8í˜¸ì„ ']):
                return 1.2  # ê¸°ë³¸ í˜¸ì„ 
            return 1.0  # ê¸°íƒ€
            
        sub['weight'] = sub['í˜¸ì„ '].fillna('').apply(get_line_weight)  # ê° ì—­ì— ê°€ì¤‘ì¹˜ ë¶€ì—¬
        
        # ê°€ì¤‘ì¹˜ ê³„ì‚°: 1200m ë°˜ê²½ ë‚´ ê°€ì¤‘ í•©ê³„
        radius_sub_max = 1200 / 6371000  # 1200m â†’ ë¼ë””ì•ˆ
        
        # Group 1: High (2.0) - ê°•ë‚¨ í•µì‹¬ í˜¸ì„ 
        sub_high = sub[sub['weight'] == 2.0]
        tree_high = BallTree(np.radians(sub_high[['ìœ„ë„', 'ê²½ë„']].values), metric='haversine')
        cnt_high = tree_high.query_radius(data_rad, r=radius_sub_max, count_only=True)
        
        # Group 2: Mid (1.5) - ì£¼ìš” í˜¸ì„ 
        sub_mid = sub[sub['weight'] == 1.5]
        tree_mid = BallTree(np.radians(sub_mid[['ìœ„ë„', 'ê²½ë„']].values), metric='haversine')
        cnt_mid = tree_mid.query_radius(data_rad, r=radius_sub_max, count_only=True)
        
        # Group 3: Low (Others) - ê¸°íƒ€ í˜¸ì„ 
        sub_low = sub[~sub.index.isin(sub_high.index) & ~sub.index.isin(sub_mid.index)]
        tree_low = BallTree(np.radians(sub_low[['ìœ„ë„', 'ê²½ë„']].values), metric='haversine')
        cnt_low = tree_low.query_radius(data_rad, r=radius_sub_max, count_only=True)
        
        # ìµœì¢… ì ìˆ˜ = (ê³ ê¸‰ í˜¸ì„  ê°œìˆ˜ * 2.0) + (ì¤‘ê¸‰ í˜¸ì„  ê°œìˆ˜ * 1.5) + (ê¸°íƒ€ í˜¸ì„  ê°œìˆ˜ * 1.0)
        data['sub_score_1200'] = (cnt_high * 2.0) + (cnt_mid * 1.5) + (cnt_low * 1.0)
        
        print(f"  âœ… ì¶”ê°€ëœ í”¼ì²˜: dist_to_bus/subway, bus_cnt_(300/500/800), sub_cnt_(300/500/800/1200), sub_score_1200")
        
    except Exception as e:
        # êµí†µ í”¼ì²˜ ì¶”ê°€ ì‹¤íŒ¨ ì‹œ 0ìœ¼ë¡œ ì±„ìš°ê¸° (ëª¨ë¸ í•™ìŠµì€ ê³„ì† ì§„í–‰)
        print(f"âš ï¸ êµí†µ í”¼ì²˜ ì¶”ê°€ ì‹¤íŒ¨: {e}")
        new_cols = ['dist_to_bus', 'dist_to_subway', 'sub_score_1200'] + \
                   [f'bus_cnt_{r}' for r in [300, 500, 800]] + \
                   [f'sub_cnt_{r}' for r in [300, 500, 800, 1200]]
        for c in new_cols:
            data[c] = 0  # ê¸°ë³¸ê°’ 0

    # ----------------------------------------------------------------
    # Step 8: íšŒì „ ì¢Œí‘œê³„ (45ë„ íšŒì „)
    # ----------------------------------------------------------------
    # [ì„¤ëª…] ì„œìš¸ì˜ ì§€ë¦¬ì  íŒ¨í„´ì€ ëŒ€ê°ì„  ë°©í–¥ì…ë‹ˆë‹¤ (ë¶ë™-ë‚¨ì„œ).
    # - ê°•ë‚¨: ë‚¨ë™ìª½
    # - ê°•ë¶: ë¶ì„œìª½
    # 
    # [í•´ê²°ì±…] 45ë„ íšŒì „ ë³€í™˜ìœ¼ë¡œ ëŒ€ê°ì„  íŒ¨í„´ì„ ìˆ˜í‰/ìˆ˜ì§ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    # - íšŒì „X = ìœ„ë„ + ê²½ë„ (ëŒ€ê°ì„  ë°©í–¥)
    # - íšŒì „Y = ìœ„ë„ - ê²½ë„ (ìˆ˜ì§ ë°©í–¥)
    # 
    # [íš¨ê³¼] ëª¨ë¸ì´ ì„œìš¸ì˜ ëŒ€ê°ì„  ê°€ê²© íŒ¨í„´ì„ ë” ì˜ í•™ìŠµí•©ë‹ˆë‹¤.
    if 'ê²½ë„' in data.columns and 'ìœ„ë„' in data.columns:
        data['íšŒì „ì¢Œí‘œX'] = data['ìœ„ë„'] + data['ê²½ë„']  # ëŒ€ê°ì„  ë°©í–¥
        data['íšŒì „ì¢Œí‘œY'] = data['ìœ„ë„'] - data['ê²½ë„']  # ìˆ˜ì§ ë°©í–¥

    # ----------------------------------------------------------------
    # Step 9: Train/Test ë¶„ë¦¬ (Data Leakage ë°©ì§€)
    # ----------------------------------------------------------------
    # [ì¤‘ìš”] ì—¬ê¸°ì„œ Train/Testë¥¼ ë¶„ë¦¬í•˜ì—¬ Data Leakageë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.
    # - K-Means: Trainìœ¼ë¡œë§Œ Fit, TestëŠ” Predict
    # - Imputation: Train Medianìœ¼ë¡œë§Œ ê³„ì‚°, Testë„ ë™ì¼í•œ ê°’ ì‚¬ìš©
    # - Label Encoding: Train ê³ ìœ ê°’ìœ¼ë¡œë§Œ ë§¤í•‘, TestëŠ” Unknown ì²˜ë¦¬
    train_part = data[data['is_train'] == 1].copy()  # Train ë°ì´í„° ì¶”ì¶œ
    test_part = data[data['is_train'] == 0].copy()   # Test ë°ì´í„° ì¶”ì¶œ
    
    # ----------------------------------------------------------------
    # Step 10: K-Means í´ëŸ¬ìŠ¤í„°ë§ (1000ê°œ í´ëŸ¬ìŠ¤í„°)
    # ----------------------------------------------------------------
    # [ì„¤ëª…] ìœ„ê²½ë„ ì¢Œí‘œë¥¼ 1000ê°œì˜ ê·¸ë£¹ìœ¼ë¡œ ë¶„ë¥˜í•˜ì—¬ ì§€ë¦¬ì  íŒ¨í„´ì„ í•™ìŠµí•©ë‹ˆë‹¤.
    # - ê°™ì€ í´ëŸ¬ìŠ¤í„° = ë¹„ìŠ·í•œ ìœ„ì¹˜ = ë¹„ìŠ·í•œ ê°€ê²© íŒ¨í„´
    # - 1000ê°œ: ì„œìš¸ ì „ì²´ë¥¼ ì„¸ë°€í•˜ê²Œ ë¶„í•  (ì•½ 110ë§Œ ê±´ / 1000 = í´ëŸ¬ìŠ¤í„°ë‹¹ 1100ê±´)
    # 
    # [ì¤‘ìš”] Trainìœ¼ë¡œë§Œ Fit, TestëŠ” Predict (Data Leakage ë°©ì§€)
    # - MiniBatchKMeans: ëŒ€ìš©ëŸ‰ ë°ì´í„°ì— ë¹ ë¥¸ í´ëŸ¬ìŠ¤í„°ë§
    # - batch_size=4096: í•œ ë²ˆì— 4096ê±´ì”© ì²˜ë¦¬
    if 'ê²½ë„' in train_part.columns and 'ìœ„ë„' in train_part.columns:
        print("ğŸ“ MiniBatchKMeans ì‹¤í–‰ ì¤‘ (k=1000, Trainë§Œ)...")
        coords_tr = train_part[['ê²½ë„', 'ìœ„ë„']].fillna(0).values  # Train ì¢Œí‘œ
        coords_te = test_part[['ê²½ë„', 'ìœ„ë„']].fillna(0).values   # Test ì¢Œí‘œ
        
        kmeans = MiniBatchKMeans(
            n_clusters=1000,      # 1000ê°œ í´ëŸ¬ìŠ¤í„°
            random_state=42,      # ì¬í˜„ì„± ë³´ì¥
            batch_size=4096,      # ë°°ì¹˜ í¬ê¸°
            n_init=10             # ì´ˆê¸°í™” íšŸìˆ˜
        )
        train_part['í´ëŸ¬ìŠ¤í„°'] = kmeans.fit_predict(coords_tr).astype(str)  # Train: Fit + Predict
        test_part['í´ëŸ¬ìŠ¤í„°'] = kmeans.predict(coords_te).astype(str)        # Test: Predictë§Œ
        
    # ----------------------------------------------------------------
    # Step 11: ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (Imputation)
    # ----------------------------------------------------------------
    # [ì„¤ëª…] ê²°ì¸¡ì¹˜ë¥¼ ì±„ì›Œì„œ ëª¨ë¸ì´ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.
    # 
    # [ì „ëµ]
    # - ìˆ«ìí˜•: Train Medianìœ¼ë¡œ ì±„ìš°ê¸° (í‰ê· ë³´ë‹¤ ì´ìƒì¹˜ì— ê°•ê±´)
    # - ë²”ì£¼í˜•: 'Unknown'ìœ¼ë¡œ ì±„ìš°ê¸° (Label Encodingì—ì„œ ì²˜ë¦¬)
    # 
    # [ì¤‘ìš”] Train Medianë§Œ ì‚¬ìš© (Data Leakage ë°©ì§€)
    # - Testì˜ ê²°ì¸¡ì¹˜ë„ Train Medianìœ¼ë¡œ ì±„ì›€
    print("ğŸ”§ ìˆ«ìí˜• ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì¤‘ (Train Median)...")
    num_cols = train_part.select_dtypes(include=[np.number]).columns  # ìˆ«ìí˜• ì»¬ëŸ¼
    cols_to_fill = [c for c in num_cols if c != target_col and c != 'í‰ë‹¹ê°€' and c != 'is_train']  # íƒ€ê²Ÿ ì œì™¸
    
    for col in cols_to_fill:
        med = train_part[col].median()  # Train Median ê³„ì‚°
        train_part[col] = train_part[col].fillna(med)  # Train ì±„ìš°ê¸°
        test_part[col] = test_part[col].fillna(med)    # Testë„ ë™ì¼í•œ ê°’ìœ¼ë¡œ ì±„ìš°ê¸°
        meta['imputation'][col] = f"Median ({med:.2f})"
        
    # ë²”ì£¼í˜• ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    cat_cols = train_part.select_dtypes(include=['object']).columns  # ë¬¸ìì—´ ì»¬ëŸ¼
    for col in cat_cols:
        train_part[col] = train_part[col].fillna('Unknown')  # 'Unknown'ìœ¼ë¡œ ì±„ìš°ê¸°
        test_part[col] = test_part[col].fillna('Unknown')
        meta['imputation'][col] = "Unknown"

    # ----------------------------------------------------------------
    # Step 12: Label Encoding (ë²”ì£¼í˜• ë³€ìˆ˜ â†’ ìˆ«ì)
    # ----------------------------------------------------------------
    # [ì„¤ëª…] ë¬¸ìì—´ ë³€ìˆ˜ë¥¼ ìˆ«ìë¡œ ë³€í™˜í•˜ì—¬ ëª¨ë¸ì´ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.
    # 
    # [ëŒ€ìƒ ë³€ìˆ˜]
    # - 'ì‹œ', 'êµ¬', 'ë™': í–‰ì •êµ¬ì—­ (ê°€ì¥ ì¤‘ìš”!)
    # - 'ì•„íŒŒíŠ¸ëª…', 'ë„ë¡œëª…': ìœ„ì¹˜ ì‹ë³„ì
    # - 'í´ëŸ¬ìŠ¤í„°': K-Means ê²°ê³¼
    # 
    # [ì¤‘ìš”] Train ê³ ìœ ê°’ìœ¼ë¡œë§Œ ë§¤í•‘ (Data Leakage ë°©ì§€)
    # - Testì— ìƒˆë¡œìš´ ê°’ì´ ë‚˜ì˜¤ë©´ 'Unknown'ìœ¼ë¡œ ì²˜ë¦¬
    print("ğŸ·ï¸ Label Encoding ì¤‘ (Train ê¸°ì¤€)...")
    encoding_features = ['ì‹œ', 'êµ¬', 'ë™', 'ì•„íŒŒíŠ¸ëª…', 'ë„ë¡œëª…', 'í´ëŸ¬ìŠ¤í„°']  # ì¸ì½”ë”© ëŒ€ìƒ
    
    for col in encoding_features:
        if col in train_part.columns:
            # label_encode_train_test í•¨ìˆ˜ ì‚¬ìš© (Train ê¸°ì¤€ ë§¤í•‘)
            train_part[col], test_part[col] = label_encode_train_test(train_part[col], test_part[col])
            meta['encoding'][col] = "Strict Mapping"  # ë©”íƒ€ë°ì´í„° ê¸°ë¡
            
    # ----------------------------------------------------------------
    # Step 13: Feature Selection (ìµœì¢… íŠ¹ì„± ì„ íƒ)
    # ----------------------------------------------------------------
    # [ì„¤ëª…] ëª¨ë¸ í•™ìŠµì— ì‚¬ìš©í•  ìµœì¢… íŠ¹ì„±ì„ ì„ íƒí•©ë‹ˆë‹¤.
    # 
    # [ì œê±° ëŒ€ìƒ]
    # - ë¬¸ìì—´ ì»¬ëŸ¼: Label Encodingì´ ì•ˆ ëœ ë¬¸ìì—´ (ëª¨ë¸ì´ í•™ìŠµ ë¶ˆê°€)
    # - 'is_train': ë°ì´í„° ë¶„ë¦¬ìš© í”Œë˜ê·¸ (íŠ¹ì„± ì•„ë‹˜)
    # - 'ê±°ë˜ê¸ˆì•¡', 'í‰ë‹¹ê°€': íƒ€ê²Ÿ ë³€ìˆ˜ (íŠ¹ì„±ì— í¬í•¨í•˜ë©´ Data Leakage)
    # 
    # [ìµœì¢… íŠ¹ì„±]
    # - ì§€ë¦¬: 'êµ¬', 'ë™', 'ê²½ë„', 'ìœ„ë„', 'íšŒì „ì¢Œí‘œX/Y', 'í´ëŸ¬ìŠ¤í„°'
    # - êµí†µ: 'dist_to_bus/subway', 'bus_cnt_*', 'sub_cnt_*', 'sub_score_1200'
    # - ê±´ë¬¼: 'ì „ìš©ë©´ì ', 'ê±´ì¶•ë…„ë„', 'ì—°ì‹', 'ì¸µ', 'ì „ì²´ì„¸ëŒ€ìˆ˜' ë“±
    # - ì‹œê°„: 'ê±°ë˜ë…„ë„', 'ê±°ë˜ì›”'
    
    # ë¶ˆí•„ìš”í•œ ë¬¸ìì—´ ì»¬ëŸ¼ ì œê±° (Label Encodingì´ ì•ˆ ëœ ì»¬ëŸ¼)
    object_cols = train_part.select_dtypes(include=['object']).columns
    train_part = train_part.drop(columns=object_cols)
    test_part = test_part.drop(columns=object_cols)
    
    # í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë°ì´í„° ì •ë¦¬
    X_train = train_part.drop(columns=['is_train'])  # 'is_train' í”Œë˜ê·¸ ì œê±°
    X_test = test_part.drop(columns=['is_train', target_col, 'í‰ë‹¹ê°€'], errors='ignore')  # íƒ€ê²Ÿ ë³€ìˆ˜ ì œê±°
    
    # í•™ìŠµ íƒ€ê²Ÿì„ 'í‰ë‹¹ê°€'ë¡œ ì„¤ì • (í•µì‹¬ ì „ëµ!)
    target = X_train['í‰ë‹¹ê°€']  # íƒ€ê²Ÿ ì¶”ì¶œ
    X_train = X_train.drop(columns=[target_col, 'í‰ë‹¹ê°€'])  # íƒ€ê²Ÿ ë³€ìˆ˜ ì œê±°
    
    # ë©”íƒ€ë°ì´í„°: ìµœì¢… íŠ¹ì„± ëª©ë¡ ì €ì¥
    meta['final_features'] = list(X_train.columns)
    print(f"âœ… Final feature count: {len(X_train.columns)}")
    
    # ----------------------------------------------------------------
    # Metadata ì €ì¥ (Streamlit ì•±ì—ì„œ ì‚¬ìš©)
    # ----------------------------------------------------------------
    # [ì„¤ëª…] ì „ì²˜ë¦¬ ê³¼ì •ì˜ ëª¨ë“  ì •ë³´ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    # - Streamlit ì•±ì—ì„œ ì‚¬ìš©ìì—ê²Œ ì „ì²˜ë¦¬ ê³¼ì •ì„ ì„¤ëª…í•˜ê¸° ìœ„í•¨
    # - ì œê±°ëœ ë³€ìˆ˜, ê²°ì¸¡ì¹˜ ì²˜ë¦¬, ì¸ì½”ë”© ì •ë³´ ë“± í¬í•¨
    meta['transport_params'] = {
        'bus_radii': "[300, 500, 800]",
        'subway_radii': "[300, 500, 800, 1200]",
        'subway_weights': "Line 2/9/Shinbundang=2.0, Line 3/5/7=1.5, Others=1.0"
    }
    meta['validation_strategy'] = "Time Series Split (Last 3 Months from dataset end)"
    meta['target_info'] = "Unit Price (ê±°ë˜ê¸ˆì•¡ / ì „ìš©ë©´ì )"
    
    # JSON íŒŒì¼ë¡œ ì €ì¥
    with open('preprocessing_metadata.json', 'w', encoding='utf-8') as f:
        json.dump(meta, f, ensure_ascii=False, indent=4)
    print("ğŸ’¾ 'preprocessing_metadata.json' ì €ì¥ ì™„ë£Œ")
    
    return X_train, target, X_test

# ==================================================================================
# 5. ëª¨ë¸ í•™ìŠµ í•¨ìˆ˜ (XGBoost Only)
# ==================================================================================
def train_ensemble_model(X_train, y_train):
    """
    XGBoost ëª¨ë¸ í•™ìŠµ ë° ê²€ì¦ í•¨ìˆ˜
    
    [ì „ëµ ë³€ê²½]
    - ì´ˆê¸°: RF + XGBoost + LightGBM ì•™ìƒë¸” (Soft Voting)
    - ì‹¤í—˜ ê²°ê³¼: ì•™ìƒë¸”ì´ ì˜¤íˆë ¤ ì„±ëŠ¥ ì €í•˜ (ë…¸ì´ì¦ˆ ì¦ê°€)
    - ìµœì¢…: **XGBoost ë‹¨ì¼ ëª¨ë¸** (ê°€ì¥ ìš°ìˆ˜í•œ ì„±ëŠ¥)
    
    [ê²€ì¦ ì „ëµ]
    - Time Series Split: ìµœê·¼ 3ê°œì›”ì„ Validationìœ¼ë¡œ ì‚¬ìš©
    - Look-ahead Bias ë°©ì§€: ê³¼ê±° ë°ì´í„°ë¡œë§Œ í•™ìŠµ, ë¯¸ë˜ ë°ì´í„°ë¡œ ê²€ì¦
    - RMSE ê³„ì‚°: 'í‰ë‹¹ê°€' â†’ 'ê±°ë˜ê¸ˆì•¡' ë³µì› í›„ ê³„ì‚° (ì‹¤ì œ ê¸ˆì•¡ ê¸°ì¤€)
    
    Args:
        X_train (DataFrame): í•™ìŠµ íŠ¹ì„±
        y_train (Series): í•™ìŠµ íƒ€ê²Ÿ ('í‰ë‹¹ê°€')
    
    Returns:
        dict: í•™ìŠµëœ ëª¨ë¸ ë”•ì…”ë„ˆë¦¬ {'xgb': XGBRegressor}
    """
    # ì›ë³¸ ë°ì´í„° ë³´ì¡´
    train_df = X_train.copy()
    train_df['í‰ë‹¹ê°€'] = y_train
    
    # ----------------------------------------------------------------
    # Step 1: ì‹œê³„ì—´ ì •ë ¬ (Time Series Split ì¤€ë¹„)
    # ----------------------------------------------------------------
    # [ì„¤ëª…] ì‹œê°„ ìˆœì„œëŒ€ë¡œ ì •ë ¬í•˜ì—¬ ê³¼ê±° â†’ ë¯¸ë˜ ìˆœì„œë¥¼ ë³´ì¥í•©ë‹ˆë‹¤.
    train_df = train_df.sort_values(by=['ê±°ë˜ë…„ë„', 'ê±°ë˜ì›”'])
    
    y = train_df['í‰ë‹¹ê°€']
    X = train_df.drop(columns=['í‰ë‹¹ê°€'])
    
    # ----------------------------------------------------------------
    # Step 2: Validation Split (Time Series: ìµœê·¼ 3ê°œì›”)
    # ----------------------------------------------------------------
    # [ì„¤ëª…] ë¦¬ë”ë³´ë“œ ì ìˆ˜ì™€ CV ì ìˆ˜ë¥¼ ì¼ì¹˜ì‹œí‚¤ê¸° ìœ„í•´ ì‹œê³„ì—´ ë¶„í• ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    # - ìµœê·¼ 3ê°œì›”: Validation Set
    # - ë‚˜ë¨¸ì§€: Training Set
    # 
    # [ì¤‘ìš”] Random Splitì„ ì‚¬ìš©í•˜ë©´ Look-ahead Bias ë°œìƒ!
    # - ë¯¸ë˜ ë°ì´í„°ë¡œ í•™ìŠµ â†’ ê³¼ê±° ë°ì´í„° ì˜ˆì¸¡ (CV ì ìˆ˜ ì¢‹ì§€ë§Œ ë¦¬ë”ë³´ë“œ ë‚˜ë¹¨)
    print("ğŸ“… Splitting Train/Val by LAST 3 MONTHS (Time Series Split)...")
    X['ym'] = X['ê±°ë˜ë…„ë„']*100 + X['ê±°ë˜ì›”']  # ë…„ì›” í•©ì¹˜ê¸° (ì˜ˆ: 202301)
    val_cutoff = X['ym'].sort_values().unique()[-3]  # ìµœê·¼ 3ê°œì›” ì¤‘ ê°€ì¥ ì˜¤ë˜ëœ ë…„ì›”
    
    mask_val = X['ym'] >= val_cutoff  # Validation: ìµœê·¼ 3ê°œì›”
    mask_tr = ~mask_val                # Training: ë‚˜ë¨¸ì§€
    
    X_tr = X[mask_tr].drop(columns=['ym'])   # Training Set
    X_val = X[mask_val].drop(columns=['ym']) # Validation Set
    y_tr = y[mask_tr]
    y_val = y[mask_val]
    
    print(f"  âœ… Train: {len(X_tr):,} samples, Val: {len(X_val):,} samples (Cutoff: {val_cutoff})")
    
    print("\n" + "="*50)
    print("ğŸ¤– Init XGBoost Model (GPU Enabled)...")
    print("="*50)
    
    # ----------------------------------------------------------------
    # Step 3: ëª¨ë¸ ì´ˆê¸°í™” (XGBoost Only)
    # ----------------------------------------------------------------
    # [ì„¤ëª…] ì´ˆê¸°ì—ëŠ” RF + XGBoost + LightGBM ì•™ìƒë¸”ì„ ì‚¬ìš©í–ˆìœ¼ë‚˜,
    # ì‹¤í—˜ ê²°ê³¼ XGBoost ë‹¨ì¼ ëª¨ë¸ì´ ê°€ì¥ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì„œ ìµœì¢… ì„ íƒí–ˆìŠµë‹ˆë‹¤.
    # 
    # [ì´ìœ ]
    # - RF: ê° íŠ¸ë¦¬ê°€ ë…ë¦½ì  â†’ ì”ì—¬ ì˜¤ì°¨ í•™ìŠµ ì•½í•¨
    # - XGBoost: Gradient Boosting â†’ ì”ì—¬ ì˜¤ì°¨ë¥¼ ì§‘ìš”í•˜ê²Œ í•™ìŠµ â†’ ì„±ëŠ¥ ìš°ìˆ˜
    # - LightGBM: ë¹ ë¥´ì§€ë§Œ XGBoostë³´ë‹¤ ì„±ëŠ¥ ë‚®ìŒ
    # - ì•™ìƒë¸”: ëª¨ë¸ ê°„ ì˜ˆì¸¡ íŒ¨í„´ ì°¨ì´ë¡œ ë…¸ì´ì¦ˆ ì¦ê°€ â†’ ì„±ëŠ¥ ì €í•˜
    
    # 1. Random Forest (Skipped - ì‚¬ìš© ì•ˆ í•¨)
    rf = RandomForestRegressor(
        n_estimators=140,
        max_features=0.5,
        random_state=42,
        n_jobs=-1
    )
    
    # ----------------------------------------------------------------
    # 2. XGBoost (Main Model) - íŠœë‹ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì‚¬ìš©
    # ----------------------------------------------------------------
    # [ì„¤ëª…] tune_xgb.pyì—ì„œ Optunaë¡œ ì°¾ì€ ìµœì  íŒŒë¼ë¯¸í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    # - RMSE 15,531 ë‹¬ì„±í•œ ìµœì  íŒŒë¼ë¯¸í„°
    # - íŒŒì¼ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
    print("  - XGBoost: ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¡œë“œ ì¤‘...")
    
    try:
        import json
        with open('best_xgb_params.json', 'r') as f:
            best_params = json.load(f)
        print(f"  âœ… 'best_xgb_params.json' ë¡œë“œ ì™„ë£Œ!")
        print(f"     - n_estimators: {best_params.get('n_estimators', 5000)}")
        print(f"     - learning_rate: {best_params.get('learning_rate', 0.01):.4f}")
        print(f"     - max_depth: {best_params.get('max_depth', 10)}")
    except FileNotFoundError:
        print("  âš ï¸ 'best_xgb_params.json' ì—†ìŒ â†’ ê¸°ë³¸ê°’ ì‚¬ìš©")
        best_params = {}
    
    xgb = XGBRegressor(
        n_estimators=best_params.get('n_estimators', 5000),
        learning_rate=best_params.get('learning_rate', 0.01),
        max_depth=best_params.get('max_depth', 10),
        min_child_weight=best_params.get('min_child_weight', 1),
        subsample=best_params.get('subsample', 0.8),
        colsample_bytree=best_params.get('colsample_bytree', 0.8),
        reg_alpha=best_params.get('reg_alpha', 0),
        reg_lambda=best_params.get('reg_lambda', 1),
        gamma=best_params.get('gamma', 0),
        random_state=42,
        n_jobs=-1,
        enable_categorical=False,
        tree_method='hist',
        device='cuda'  # GPU ì‚¬ìš©
    )
    
    # 3. LightGBM (Skipped - ì‚¬ìš© ì•ˆ í•¨)
    # ì£¼ì˜: LightGBM GPU ë²„ì „ì´ ì„¤ì¹˜ë˜ì–´ ìˆì–´ì•¼ í•¨. ì—ëŸ¬ ë°œìƒ ì‹œ device='cpu'ë¡œ ë³€ê²½ í•„ìš”.
    lgbm = LGBMRegressor(
        n_estimators=1000,
        learning_rate=0.01,
        num_leaves=127,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        n_jobs=-1,
        verbose=-1,
        device='gpu'
    )
    
    # ----------------------------------------------------------------
    # Step 4: ëª¨ë¸ í•™ìŠµ ë° Validation ì˜ˆì¸¡
    # ----------------------------------------------------------------
    # [ì„¤ëª…] XGBoostë§Œ í•™ìŠµí•˜ê³ , RFì™€ LightGBMì€ ìŠ¤í‚¨í•©ë‹ˆë‹¤.
    # - XGBoost Only Mode: ê°€ì¤‘ì¹˜ [0.0, 1.0, 0.0]
    
    # Store models and predictions
    models = {}
    val_preds = {}
    test_preds = {}
    
    # 4.1 RandomForest (Skipped)
    print("\n[1/3] RandomForest: ìŠ¤í‚µ (XGBoost Only ëª¨ë“œ)")
    val_preds['rf'] = np.zeros(len(X_val))  # Dummy ì˜ˆì¸¡ (0ìœ¼ë¡œ ì±„ì›€)
    models['rf'] = rf
    
    # 4.2 XGBoost (Main Model)
    print("\n[2/3] XGBoost í•™ìŠµ ì¤‘ (GPU)...")
    xgb.fit(
        X_tr, y_tr,
        eval_set=[(X_val, y_val)],  # Validation Setìœ¼ë¡œ ì¡°ê¸° ì¢…ë£Œ íŒë‹¨
        verbose=100  # 100 iterationë§ˆë‹¤ ì§„í–‰ ìƒí™© ì¶œë ¥
    )
    val_preds['xgb'] = xgb.predict(X_val)  # Validation ì˜ˆì¸¡
    models['xgb'] = xgb
    
    # 4.3 LightGBM (Skipped)
    print("\n[3/3] LightGBM: ìŠ¤í‚µ (XGBoost Only ëª¨ë“œ)")
    val_preds['lgbm'] = np.zeros(len(X_val))  # Dummy ì˜ˆì¸¡
    models['lgbm'] = lgbm

    # ----------------------------------------------------------------
    # Step 5: RMSE ê³„ì‚° (Validation Set)
    # ----------------------------------------------------------------
    # [ì„¤ëª…] ëª¨ë¸ì€ 'í‰ë‹¹ê°€'ë¥¼ ì˜ˆì¸¡í•˜ì§€ë§Œ, RMSEëŠ” 'ê±°ë˜ê¸ˆì•¡' ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°í•©ë‹ˆë‹¤.
    # - í‰ë‹¹ê°€ â†’ ê±°ë˜ê¸ˆì•¡ ë³µì›: í‰ë‹¹ê°€ * ì „ìš©ë©´ì 
    # - RMSE = sqrt(mean((y_true - y_pred)^2))
    # 
    # [ì¤‘ìš”] ë¦¬ë”ë³´ë“œì—ì„œë„ 'ê±°ë˜ê¸ˆì•¡' ê¸°ì¤€ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤!
    val_area = X_val['ì „ìš©ë©´ì '].values  # Validation Setì˜ ë©´ì 
    y_val_total = y_val.values * val_area  # ì‹¤ì œ ê±°ë˜ê¸ˆì•¡ (ë§Œì›)

    # ê°œë³„ ëª¨ë¸ RMSE ê³„ì‚°
    rmse_rf = 0.0  # RandomForestëŠ” ì‚¬ìš© ì•ˆ í•¨
    
    rmse_xgb = np.sqrt(mean_squared_error(y_val_total, val_preds['xgb'] * val_area))
    print(f"  ğŸ‘‰ XGBoost Validation RMSE: {rmse_xgb:,.2f}")
    
    rmse_lgbm = 0.0  # LightGBMë„ ì‚¬ìš© ì•ˆ í•¨

    # ----------------------------------------------------------------
    # Step 6: ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ê²°ì •
    # ----------------------------------------------------------------
    # [ì„¤ëª…] ì„¸ ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ ê°€ì¤‘ í‰ê· í•˜ì—¬ ìµœì¢… ì˜ˆì¸¡ì„ ë§Œë“­ë‹ˆë‹¤.
    # 
    # [ì „ëµ]
    # - Manual Mode: ìˆ˜ë™ìœ¼ë¡œ ê°€ì¤‘ì¹˜ ì„¤ì • [0.0, 1.0, 0.0] = XGBoost Only
    # - Auto Mode: scipy.optimizeë¡œ ìµœì  ê°€ì¤‘ì¹˜ íƒìƒ‰ (ì‹¤í—˜ìš©)
    # 
    # [ê²°ë¡ ] XGBoost ë‹¨ì¼ ëª¨ë¸ì´ ê°€ì¥ ìš°ìˆ˜í•˜ë¯€ë¡œ Manual Mode ì‚¬ìš©
    print("\nğŸ¤– ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ê²°ì • ì¤‘...")
    
    # [ì‚¬ìš©ì ì˜µì…˜] Manual Weight Mode
    # True: MANUAL_WEIGHTS ì‚¬ìš©, False: scipy.optimize ì‚¬ìš©
    MANUAL_MODE = True
    
    # ê°€ì¤‘ì¹˜ [RandomForest, XGBoost, LightGBM]
    # í•©ì´ 1.0ì´ ë˜ë„ë¡ ì„¤ì •
    MANUAL_WEIGHTS = [0.0, 1.0, 0.0]  # XGBoost Only
    
    def ensemble_rmse(weights):
        """
        ì•™ìƒë¸” RMSE ê³„ì‚° í•¨ìˆ˜
        
        Args:
            weights (list): [RF, XGB, LGBM] ê°€ì¤‘ì¹˜
        
        Returns:
            float: Validation RMSE (ê±°ë˜ê¸ˆì•¡ ê¸°ì¤€)
        """
        # í‰ë‹¹ê°€ ê°€ì¤‘ í‰ê· 
        final_pred_unit = (
            weights[0] * val_preds['rf'] + 
            weights[1] * val_preds['xgb'] + 
            weights[2] * val_preds['lgbm']
        )
        # ê±°ë˜ê¸ˆì•¡ìœ¼ë¡œ ë³€í™˜
        final_pred_total = final_pred_unit * val_area
        return np.sqrt(mean_squared_error(y_val_total, final_pred_total))

    if MANUAL_MODE:
        print(f"ğŸ”§ ìˆ˜ë™ ëª¨ë“œ í™œì„±í™”. ë¯¸ë¦¬ ì„¤ì •ëœ ê°€ì¤‘ì¹˜ ì‚¬ìš©: {MANUAL_WEIGHTS}")
        best_weights = np.array(MANUAL_WEIGHTS)
        best_rmse = ensemble_rmse(best_weights)
        
    else:
        # ìë™ ìµœì í™” ëª¨ë“œ (ì‹¤í—˜ìš©)
        print("âš¡ ìë™ ìµœì í™” ëª¨ë“œ í™œì„±í™”.")
        from scipy.optimize import minimize
        
        # ì´ˆê¸° ê°€ì¤‘ì¹˜: ë™ì¼ ê°€ì¤‘ì¹˜
        init_weights = [1/3, 1/3, 1/3]
        
        # ì œì•½ ì¡°ê±´: ê°€ì¤‘ì¹˜ í•© = 1
        constraints = ({'type': 'eq', 'fun': lambda w: 1 - sum(w)})
        
        # ë²”ìœ„: ê° ê°€ì¤‘ì¹˜ëŠ” 0~1 ì‚¬ì´
        bounds = [(0, 1), (0, 1), (0, 1)]
        
        # ìµœì í™” ì‹¤í–‰
        result = minimize(ensemble_rmse, init_weights, method='SLSQP', bounds=bounds, constraints=constraints)
        best_weights = result.x
        best_rmse = result.fun

    print(f"\nâœ… ìµœì¢… ê°€ì¤‘ì¹˜:")
    print(f"  - RandomForest : {best_weights[0]:.4f}")
    print(f"  - XGBoost      : {best_weights[1]:.4f}")
    print(f"  - LightGBM     : {best_weights[2]:.4f}")
    
    print(f"  ğŸ‘‰ ì•™ìƒë¸” Validation RMSE: {best_rmse:,.2f}")
    
    # ----------------------------------------------------------------
    # Step 7: ë©”íŠ¸ë¦­ ì €ì¥ (Streamlit ì•±ìš©)
    # ----------------------------------------------------------------
    # [ì„¤ëª…] ëª¨ë¸ ì„±ëŠ¥ ë©”íŠ¸ë¦­ì„ JSON íŒŒì¼ë¡œ ì €ì¥í•˜ì—¬ Streamlit ì•±ì—ì„œ í‘œì‹œí•©ë‹ˆë‹¤.
    import json
    from datetime import datetime
    
    ensemble_metrics = {
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "ensemble_rmse": best_rmse,
        "individual_rmse": {
            "RandomForest": rmse_rf,
            "XGBoost": rmse_xgb,
            "LightGBM": rmse_lgbm
        },
        "optimal_weights": {
            "RandomForest": best_weights[0],
            "XGBoost": best_weights[1],
            "LightGBM": best_weights[2]
        }
    }
    
    with open('ensemble_metrics.json', 'w', encoding='utf-8') as f:
        json.dump(ensemble_metrics, f, ensure_ascii=False, indent=4)
    print("âœ… 'ensemble_metrics.json' ë©”íŠ¸ë¦­ ì €ì¥ ì™„ë£Œ")
    
    # ----------------------------------------------------------------
    # Step 8: ì „ì²´ ë°ì´í„°ë¡œ ì¬í•™ìŠµ (Final Submissionìš©)
    # ----------------------------------------------------------------
    # [ì„¤ëª…] Validation Setì„ í¬í•¨í•œ ì „ì²´ ë°ì´í„°ë¡œ ë‹¤ì‹œ í•™ìŠµí•˜ì—¬ ì„±ëŠ¥ì„ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤.
    # - Validation Splitì€ ëª¨ë¸ í‰ê°€ìš©ì´ì—ˆê³ , ìµœì¢… ì œì¶œì€ ëª¨ë“  ë°ì´í„° ì‚¬ìš©
    # - ë” ë§ì€ ë°ì´í„° = ë” ë‚˜ì€ ì„±ëŠ¥
    print("\nğŸš€ ì „ì²´ ë°ì´í„°ë¡œ ì¬í•™ìŠµ ì¤‘ (Final Submissionìš©)...")
    
    # [Leakage Prevention] 'ym' ì»¬ëŸ¼ ì œê±° (ì‹œê³„ì—´ ë¶„í• ìš©ì´ì—ˆìŒ)
    if 'ym' in X.columns:
        X = X.drop(columns=['ym'])
    
    # ê°€ì¤‘ì¹˜ê°€ 0ë³´ë‹¤ í° ëª¨ë¸ë§Œ ì¬í•™ìŠµ (XGBoostë§Œ í•™ìŠµ)
    if best_weights[0] > 0:
        print("  - RandomForest ì¬í•™ìŠµ ì¤‘...")
        rf.fit(X, y)
    
    if best_weights[1] > 0:
        print("  - XGBoost ì¬í•™ìŠµ ì¤‘...")
        xgb.fit(X, y)  # ì „ì²´ ë°ì´í„°ë¡œ í•™ìŠµ
        
    if best_weights[2] > 0:
        print("  - LightGBM ì¬í•™ìŠµ ì¤‘...")
        lgbm.fit(X, y)
    
    # ----------------------------------------------------------------
    # Step 9: Feature Importance ì¶”ì¶œ ë° ì €ì¥
    # ----------------------------------------------------------------
    # [ì„¤ëª…] XGBoostì˜ Feature Importanceë¥¼ ì¶”ì¶œí•˜ì—¬ Streamlit ì•±ì—ì„œ í‘œì‹œí•©ë‹ˆë‹¤.
    # - Feature Importance: ê° íŠ¹ì„±ì´ ëª¨ë¸ ì˜ˆì¸¡ì— ì–¼ë§ˆë‚˜ ê¸°ì—¬í•˜ëŠ”ì§€
    # - ìƒìœ„ íŠ¹ì„±: 'êµ¬', 'ìœ„ë„', 'ê²½ë„', 'ì—°ì‹' ë“±
    try:
        if best_weights[1] > 0:  # XGBoost ì‚¬ìš© ì‹œ
            print("ğŸ“Š Feature Importance ì¶”ì¶œ ë° ì €ì¥ (XGBoost)...")
            fi = xgb.feature_importances_  # Feature Importance ë°°ì—´
            fi_df = pd.DataFrame({'feature': X.columns, 'importance': fi})
            fi_df = fi_df.sort_values(by='importance', ascending=False)
            fi_df.to_csv('feature_importance.csv', index=False, encoding='utf-8-sig')
            print("âœ… 'feature_importance.csv' ì €ì¥ ì™„ë£Œ")
        elif best_weights[0] > 0:  # RF fallback (ì‚¬ìš© ì•ˆ í•¨)
            print("ğŸ“Š Feature Importance ì¶”ì¶œ ë° ì €ì¥ (RandomForest)...")
            fi = rf.feature_importances_
            fi_df = pd.DataFrame({'feature': X.columns, 'importance': fi})
            fi_df = fi_df.sort_values(by='importance', ascending=False)
            fi_df.to_csv('feature_importance.csv', index=False, encoding='utf-8-sig')
            print("âœ… 'feature_importance.csv' ì €ì¥ ì™„ë£Œ")
    except Exception as e:
        print(f"âš ï¸ Feature Importance ì €ì¥ ì‹¤íŒ¨: {e}")

    return models, best_weights

# ==================================================================================
# 6. ì˜ˆì¸¡ ë° ì œì¶œ íŒŒì¼ ìƒì„± í•¨ìˆ˜
# ==================================================================================
def make_submission(models, best_weights, X_test):
    """
    ì œì¶œ íŒŒì¼ ìƒì„± í•¨ìˆ˜
    
    [ì„¤ëª…]
    í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ Test ë°ì´í„°ë¥¼ ì˜ˆì¸¡í•˜ê³  ì œì¶œ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    [ì˜ˆì¸¡ ê³¼ì •]
    1. ê° ëª¨ë¸ë¡œ 'í‰ë‹¹ê°€' ì˜ˆì¸¡
    2. ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ìµœì¢… 'í‰ë‹¹ê°€' ê³„ì‚°
    3. 'í‰ë‹¹ê°€' Ã— 'ì „ìš©ë©´ì ' = 'ê±°ë˜ê¸ˆì•¡' ë³µì›
    4. ì •ìˆ˜ ë³€í™˜ í›„ CSV ì €ì¥
    
    Args:
        models (dict): í•™ìŠµëœ ëª¨ë¸ {'rf', 'xgb', 'lgbm'}
        best_weights (array): ìµœì  ê°€ì¤‘ì¹˜ [RF, XGB, LGBM]
        X_test (DataFrame): í…ŒìŠ¤íŠ¸ íŠ¹ì„±
    """
    print("\nğŸ”® í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ì¤‘...")
    
    # ìµœì¢… ì˜ˆì¸¡ê°’ ì´ˆê¸°í™”
    final_pred_unit = np.zeros(len(X_test))
    
    # 1. Random Forest (ê°€ì¤‘ì¹˜ 0ì´ë©´ ìŠ¤í‚µ)
    if best_weights[0] > 0:
        print(f"  - RandomForest ì˜ˆì¸¡ ì¤‘ (ê°€ì¤‘ì¹˜: {best_weights[0]:.2f})...")
        final_pred_unit += best_weights[0] * models['rf'].predict(X_test)
        
    # 2. XGBoost (Main Model)
    if best_weights[1] > 0:
        print(f"  - XGBoost ì˜ˆì¸¡ ì¤‘ (ê°€ì¤‘ì¹˜: {best_weights[1]:.2f})...")
        final_pred_unit += best_weights[1] * models['xgb'].predict(X_test)
        
    # 3. LightGBM (ê°€ì¤‘ì¹˜ 0ì´ë©´ ìŠ¤í‚µ)
    if best_weights[2] > 0:
        print(f"  - LightGBM ì˜ˆì¸¡ ì¤‘ (ê°€ì¤‘ì¹˜: {best_weights[2]:.2f})...")
        final_pred_unit += best_weights[2] * models['lgbm'].predict(X_test)
    
    # í‰ë‹¹ê°€ â†’ ê±°ë˜ê¸ˆì•¡ ë³µì›
    test_area = X_test['ì „ìš©ë©´ì ']
    pred_total = final_pred_unit * test_area
    
    # ì •ìˆ˜í˜• ë³€í™˜ í›„ CSV ì €ì¥
    submission = pd.DataFrame({'target': pred_total.astype(int)})
    submission.to_csv('submission_ensemble_weighted.csv', index=False)
    print("\nâœ… 'submission_ensemble_weighted.csv' ì œì¶œ íŒŒì¼ ì €ì¥ ì™„ë£Œ!")

# ==================================================================================
# 7. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# ==================================================================================
def main():
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
    
    [ì‹¤í–‰ ìˆœì„œ]
    1. ë°ì´í„° ë¡œë“œ (train.csv, test.csv)
    2. ì „ì²˜ë¦¬ ë° íŠ¹ì„± ê³µí•™
    3. XGBoost ëª¨ë¸ í•™ìŠµ
    4. ì œì¶œ íŒŒì¼ ìƒì„±
    """
    print("\n" + "="*60)
    print("ğŸš€ ì„œìš¸ ì•„íŒŒíŠ¸ ì‹¤ê±°ë˜ê°€ ì˜ˆì¸¡ - XGBoost ëª¨ë¸ í•™ìŠµ ì‹œì‘")
    print("="*60)
    
    # 1. ë°ì´í„° ë¡œë“œ
    print("\nğŸ“ [1/4] ë°ì´í„° ë¡œë“œ ì¤‘...")
    train_org, test_org = load_data()
    
    # 2. ì „ì²˜ë¦¬ (íŠ¹ì„± ê³µí•™ í¬í•¨)
    print("\nâš™ï¸ [2/4] ë°ì´í„° ì „ì²˜ë¦¬ ë° íŠ¹ì„± ê³µí•™...")
    X_train, target, X_test = preprocess_data(train_org.copy(), test_org.copy())
    
    # 3. ëª¨ë¸ í•™ìŠµ
    print("\nğŸ¤– [3/4] XGBoost ëª¨ë¸ í•™ìŠµ ì¤‘...")
    models, best_weights = train_ensemble_model(X_train, target)
    
    # 4. ì œì¶œ íŒŒì¼ ìƒì„±
    print("\nğŸ“¤ [4/4] ì œì¶œ íŒŒì¼ ìƒì„± ì¤‘...")
    make_submission(models, best_weights, X_test)
    
    print("\n" + "="*60)
    print("ğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
    print("="*60)


if __name__ == '__main__':
    main()
